{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:35.217379Z",
     "start_time": "2021-07-25T13:55:32.998230Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import logging\n",
    "import collections\n",
    "\n",
    "# 特别注意，这个是bert的tokenization包，而不是pip安装得到的，具体可以参见bert的官方文档\n",
    "# https://github.com/google-research/bert\n",
    "#　https://github.com/google-research/bert/blob/master/tokenization.py\n",
    "\n",
    "import tokenization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import jieba  # 中文分词库，百度员工开发\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)  # 设置np显示数值而不是科学计数法\n",
    "# 查询系统可用的 GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# 确保有可用的 GPU 如果没有, 则会报错\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# 设置参数,该段务必在运行jupyter的第一段代码执行，否则会无法初始化成功\n",
    "# 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:40.248061Z",
     "start_time": "2021-07-25T13:55:40.242735Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '/data/python/tensorflow/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:40.858379Z",
     "start_time": "2021-07-25T13:55:40.820493Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path + 'train_20200228.csv')\n",
    "val_df = pd.read_csv(path + 'dev_20200228.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['咳血', 1],\n",
       "       ['咳血', 1],\n",
       "       ['咳血', 0],\n",
       "       ...,\n",
       "       ['哮喘', 0],\n",
       "       ['哮喘', 0],\n",
       "       ['哮喘', 0]], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "train_df[['category','label']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 | ['咳血' 1]\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(train_df[['category','label']].values):\n",
    "    print(i,'|', val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:41.183814Z",
     "start_time": "2021-07-25T13:55:41.161935Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id category         query1           query2  label\n",
       "0   0       咳血  剧烈运动后咯血,是怎么了?    剧烈运动后咯血是什么原因？      1\n",
       "1   1       咳血  剧烈运动后咯血,是怎么了?     剧烈运动后为什么会咯血？      1\n",
       "2   2       咳血  剧烈运动后咯血,是怎么了?  剧烈运动后咯血，应该怎么处理？      0\n",
       "3   3       咳血  剧烈运动后咯血,是怎么了?   剧烈运动后咯血，需要就医吗？      0\n",
       "4   4       咳血  剧烈运动后咯血,是怎么了?   剧烈运动后咯血，是否很严重？      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>category</th>\n      <th>query1</th>\n      <th>query2</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>咳血</td>\n      <td>剧烈运动后咯血,是怎么了?</td>\n      <td>剧烈运动后咯血是什么原因？</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>咳血</td>\n      <td>剧烈运动后咯血,是怎么了?</td>\n      <td>剧烈运动后为什么会咯血？</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>咳血</td>\n      <td>剧烈运动后咯血,是怎么了?</td>\n      <td>剧烈运动后咯血，应该怎么处理？</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>咳血</td>\n      <td>剧烈运动后咯血,是怎么了?</td>\n      <td>剧烈运动后咯血，需要就医吗？</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>咳血</td>\n      <td>剧烈运动后咯血,是怎么了?</td>\n      <td>剧烈运动后咯血，是否很严重？</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:41.446653Z",
     "start_time": "2021-07-25T13:55:41.435654Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['咳血', '支原体肺炎', '胸膜炎', '肺气肿', '肺炎', '感冒', '上呼吸道感染', '哮喘'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "train_df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:41.915992Z",
     "start_time": "2021-07-25T13:55:41.899965Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id category         query1              query2  label\n",
       "0   0       咳血  请问呕血与咯血有什么区别？  请问呕血与咯血这两者之间有什么区别？      1\n",
       "1   1       咳血  请问呕血与咯血有什么区别？          请问呕血与咯血异同？      1\n",
       "2   2       咳血  请问呕血与咯血有什么区别？        请问呕血与咯血怎么治疗？      0\n",
       "3   3       咳血  请问呕血与咯血有什么区别？    请问呕血与咯血是什么原因导致的？      0\n",
       "4   4       咳血  请问呕血与咯血有什么区别？   请问呕血与咯血与其他疾病有关联吗？      0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>category</th>\n      <th>query1</th>\n      <th>query2</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>咳血</td>\n      <td>请问呕血与咯血有什么区别？</td>\n      <td>请问呕血与咯血这两者之间有什么区别？</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>咳血</td>\n      <td>请问呕血与咯血有什么区别？</td>\n      <td>请问呕血与咯血异同？</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>咳血</td>\n      <td>请问呕血与咯血有什么区别？</td>\n      <td>请问呕血与咯血怎么治疗？</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>咳血</td>\n      <td>请问呕血与咯血有什么区别？</td>\n      <td>请问呕血与咯血是什么原因导致的？</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>咳血</td>\n      <td>请问呕血与咯血有什么区别？</td>\n      <td>请问呕血与咯血与其他疾病有关联吗？</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:42.426439Z",
     "start_time": "2021-07-25T13:55:42.416734Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['咳血', '支原体肺炎', '胸膜炎', '肺气肿', '肺炎', '感冒', '上呼吸道感染', '哮喘'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "val_df.category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换为bert 支持的数据格式\n",
    "- token_id\n",
    "- input_mask\n",
    "- segment_id\n",
    "\n",
    "query1 和 query2 合并成一个句子, 需要三个向量，cls向量，seq向量和seq向量\n",
    "\n",
    "[cls] query1 [seq] query2 [seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:43.120468Z",
     "start_time": "2021-07-25T13:55:43.051979Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入一个字典  \n",
    "tokenizer= tokenization.FullTokenizer(vocab_file =path + 'bert_zh_L-12_H-768_A-12_2/assets/vocab.txt',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:43.546023Z",
     "start_time": "2021-07-25T13:55:43.536462Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['庆', '祝', '中', '国', '共', '产', '党', '建', '党', '100', '周', '年', '！']"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# 测试一下导入的token的方法调用，将句子分割为一个个的字\n",
    "tokenizer.tokenize(\"庆祝中国共产党建党100周年！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:55:44.150226Z",
     "start_time": "2021-07-25T13:55:44.140271Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2412, 4867, 704, 1744, 1066, 772, 1054, 2456, 1054, 8135, 1453, 2399, 8013]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "a = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"庆祝中国共产党建党100周年！\"))\n",
    "a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-25T13:56:46.967134Z",
     "start_time": "2021-07-25T13:56:46.958494Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[123, 125, 122, 130]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('2419')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[122]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 句子切割\n",
    "#### 两个句子整合成一个句子\n",
    "#### 句子编码 （segment_ids, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, idx, category, text_a, text_b = None, label = None):\n",
    "     self.id = idx\n",
    "     self.category = category\n",
    "     self.text_a = text_a\n",
    "     self.text_b = text_b\n",
    "     self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask 遮蔽的意思\n",
    "class InputFeatures(object):\n",
    "    def __init__(self,input_ids,input_mask,segment_ids,label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 截断序列并成为一对数据，让token_a和token_b长度一致\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment_ids 段落\n",
    "\n",
    "def convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer):\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "    \n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    # 开始处理为规范的输入格式，如果是文本分类，则第二个seq就全部为0\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "    tokens = []; segment_ids = []\n",
    "    tokens.append(\"[CLS]\"); segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "    # 上述代码完成处理成规范的格式  [cls][query1][seq][query2][seq]\n",
    "\n",
    "    # tokens转化为字id\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print('input_ids',input_ids)\n",
    "\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "    \n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    label_id = label_map[example.label]\n",
    "\n",
    "    feature = InputFeatures(input_ids=input_ids, input_mask = input_mask, segment_ids = segment_ids, label_id = label_id)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensorflow.core.example.feature_pb2.Feature"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "tf.train.Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "剧烈运动后咯血,是怎么了?\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(train_df.values):\n",
    "    text_a = tokenization.convert_to_unicode(val[2])\n",
    "    print(text_a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据为example\n",
    "def _create_examples(lines, set_type = 'train'):\n",
    "    examples = []\n",
    "    for (i, line) in enumerate(lines):\n",
    "        idx = line[0]\n",
    "        category = tokenization.convert_to_unicode(line[1])\n",
    "        text_a = tokenization.convert_to_unicode(line[2])\n",
    "        text_b = tokenization.convert_to_unicode(line[3])\n",
    "\n",
    "        if set_type == 'test':\n",
    "            label = 0\n",
    "        else:\n",
    "            label = line[4]\n",
    "        examples.append(InputExample(idx = idx, category = category, text_a = text_a, text_b = text_b, label = label))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Examples = _create_examples(train_df.values, set_type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一个TFrecord数据格式\n",
    "def file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "    \"\"\"\n",
    "    output_file:输出文件\n",
    "    \"\"\"\n",
    "    writer = tf.io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logging.info(f'Writing example {ex_index} of {len(examples)}')\n",
    "\n",
    "        feature = convert_single_example(ex_index, example, label_list, max_seq_length, tokenizer)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list = tf.train.Int64List(value = list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features['input_ids'] = create_int_feature(feature.input_ids)\n",
    "        features['input_mask'] = create_int_feature(feature.input_mask)\n",
    "        features['segment_ids'] = create_int_feature(feature.segment_ids)\n",
    "        features['label_ids'] = create_int_feature([feature.label_id])\n",
    "\n",
    "        tf_example = tf.train.Example(features = tf.train.Features(feature =  features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path + \"preprocess_data\"):\n",
    "    os.mkdir(path + \"preprocess_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ", 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 1355, 868, 679, 1391, 5790, 5543, 2806, 6814, 1343, 1408, 102, 6814, 3130, 2595, 1527, 1596, 3300, 784, 720, 3175, 3791, 3780, 4545, 8024, 1377, 809, 2515, 2419, 3780, 1962, 1408, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 1355, 868, 679, 1391, 5790, 5543, 2806, 6814, 1343, 1408, 102, 6814, 3130, 2595, 1527, 1596, 1355, 868, 1377, 809, 1391, 3717, 3362, 1408, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 679, 5543, 1391, 784, 720, 7608, 4289, 8043, 102, 6814, 3130, 2595, 1527, 1596, 2642, 5442, 784, 720, 7608, 4289, 679, 5543, 1391, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 679, 5543, 1391, 784, 720, 7608, 4289, 8043, 102, 6814, 3130, 2595, 1527, 1596, 2555, 1391, 784, 720, 7608, 4289, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 679, 5543, 1391, 784, 720, 7608, 4289, 8043, 102, 6814, 3130, 2595, 1527, 1596, 1355, 868, 3221, 784, 720, 4568, 4307, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 679, 5543, 1391, 784, 720, 7608, 4289, 8043, 102, 6814, 3130, 2595, 1527, 1596, 3221, 2190, 784, 720, 6814, 3130, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 679, 5543, 1391, 784, 720, 7608, 4289, 8043, 102, 6814, 3130, 2595, 1527, 1596, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6421, 2582, 720, 1215, 8043, 102, 2533, 749, 6814, 3130, 2595, 1527, 1596, 1963, 862, 3221, 1962, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6421, 2582, 720, 1215, 8043, 102, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6421, 2582, 720, 3780, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6421, 2582, 720, 1215, 8043, 102, 2533, 749, 6814, 3130, 2595, 1527, 1596, 833, 837, 3381, 1408, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6421, 2582, 720, 1215, 8043, 102, 2533, 749, 6814, 3130, 2595, 1527, 1596, 5543, 1391, 784, 720, 7608, 4289, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6421, 2582, 720, 1215, 8043, 102, 6814, 3130, 2595, 1527, 1596, 7410, 3780, 1408, 8043, 102]\n",
      "input_ids [101, 2714, 7349, 5511, 4638, 6402, 3171, 3300, 3403, 1114, 1408, 8043, 102, 2714, 7349, 5511, 4638, 6402, 3171, 3300, 3766, 3300, 3403, 1114, 8043, 102]\n",
      "input_ids [101, 2714, 7349, 5511, 4638, 6402, 3171, 3300, 3403, 1114, 1408, 8043, 102, 6402, 3171, 2714, 7349, 5511, 3300, 671, 2137, 4638, 3403, 1114, 1408, 8043, 102]\n",
      "input_ids [101, 2714, 7349, 5511, 4638, 6402, 3171, 3300, 3403, 1114, 1408, 8043, 102, 9162, 3221, 3466, 3389, 5511, 3698, 5514, 4638, 7032, 3403, 1114, 1408, 8043, 102]\n",
      "input_ids [101, 2714, 7349, 5511, 4638, 6402, 3171, 3300, 3403, 1114, 1408, 8043, 102, 711, 784, 720, 6432, 3300, 5511, 3698, 5514, 679, 7444, 6206, 3780, 4545, 8043, 102]\n",
      "input_ids [101, 2714, 7349, 5511, 4638, 6402, 3171, 3300, 3403, 1114, 1408, 8043, 102, 5511, 3698, 5514, 4638, 6402, 3171, 3403, 1114, 3221, 784, 720, 136, 102]\n",
      "input_ids [101, 2697, 1088, 833, 2471, 6629, 6814, 3130, 2595, 1527, 1596, 1408, 8043, 102, 2697, 1088, 3221, 1415, 833, 6430, 1355, 6814, 3130, 2595, 1527, 1596, 1408, 8043, 102]\n",
      "input_ids [101, 2697, 1088, 833, 2471, 6629, 6814, 3130, 2595, 1527, 1596, 1408, 8043, 102, 2697, 1088, 833, 2193, 5636, 6814, 3130, 2595, 1527, 1596, 1355, 868, 1408, 136, 102]\n",
      "input_ids [101, 2697, 1088, 833, 2471, 6629, 6814, 3130, 2595, 1527, 1596, 1408, 8043, 102, 2111, 2094, 2697, 1088, 2471, 6629, 6814, 3130, 2595, 7965, 4142, 8024, 2682, 7309, 1963, 862, 3780, 4545, 1350, 7564, 7344, 8043, 102]\n",
      "input_ids [101, 2697, 1088, 833, 2471, 6629, 6814, 3130, 2595, 1527, 1596, 1408, 8043, 102, 6814, 3130, 2595, 5166, 4620, 4638, 2111, 2094, 2697, 1088, 749, 2582, 720, 2844, 4415, 8043, 102]\n",
      "input_ids [101, 2697, 1088, 833, 2471, 6629, 6814, 3130, 2595, 1527, 1596, 1408, 8043, 102, 2111, 2094, 4385, 1762, 6814, 3130, 2595, 1495, 1644, 8024, 1377, 809, 1391, 2697, 1088, 5790, 1408, 8043, 102]\n",
      "input_ids [101, 2533, 6814, 1527, 1596, 4567, 4638, 782, 117, 3300, 1525, 763, 6844, 1394, 4638, 978, 6716, 3175, 2466, 136, 102, 3300, 1525, 763, 978, 6716, 3175, 2466, 6844, 1394, 1527, 1596, 4567, 2642, 5442, 136, 102]\n",
      "input_ids [101, 2533, 6814, 1527, 1596, 4567, 4638, 782, 117, 3300, 1525, 763, 6844, 1394, 4638, 978, 6716, 3175, 2466, 136, 102, 2533, 6814, 1527, 1596, 4567, 4638, 782, 117, 1377, 809, 6848, 2885, 2582, 3416, 4638, 978, 6716, 3175, 102]\n",
      "input_ids [101, 2533, 6814, 1527, 1596, 4567, 4638, 782, 117, 3300, 1525, 763, 6844, 1394, 4638, 978, 6716, 3175, 2466, 136, 102, 1527, 1596, 4567, 2642, 5442, 5543, 1415, 6651, 3635, 6817, 1220, 8043, 102]\n",
      "input_ids [101, 2533, 6814, 1527, 1596, 4567, 4638, 782, 117, 3300, 1525, 763, 6844, 1394, 4638, 978, 6716, 3175, 2466, 136, 102, 1527, 1596, 4567, 3221, 2582, 720, 2533, 677, 4638, 8043, 102]\n",
      "input_ids [101, 2533, 6814, 1527, 1596, 4567, 4638, 782, 117, 3300, 1525, 763, 6844, 1394, 4638, 978, 6716, 3175, 2466, 136, 102, 1527, 1596, 4567, 5543, 3780, 1962, 1408, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 4638, 3466, 3389, 3175, 3791, 6963, 3300, 1525, 763, 1557, 8043, 102, 1527, 1596, 3466, 3389, 3300, 1525, 763, 3466, 3389, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 4638, 3466, 3389, 3175, 3791, 6963, 3300, 1525, 763, 1557, 8043, 102, 1527, 1596, 3466, 3389, 7555, 4680, 3300, 1525, 763, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 4638, 3466, 3389, 3175, 3791, 6963, 3300, 1525, 763, 1557, 8043, 102, 1527, 1596, 4567, 3221, 2582, 720, 3466, 3389, 1139, 3341, 4638, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 4638, 3466, 3389, 3175, 3791, 6963, 3300, 1525, 763, 1557, 8043, 102, 1157, 3466, 3389, 6814, 3130, 3975, 3300, 2212, 6087, 8024, 1963, 862, 5564, 3130, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 4638, 3466, 3389, 3175, 3791, 6963, 3300, 1525, 763, 1557, 8043, 102, 5511, 1216, 5543, 3466, 3389, 3633, 2382, 5543, 1415, 6402, 3171, 711, 1527, 1596, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3221, 784, 720, 1333, 1728, 2193, 5636, 4638, 8043, 102, 1527, 1596, 3221, 784, 720, 1333, 1728, 2471, 6629, 4638, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3221, 784, 720, 1333, 1728, 2193, 5636, 4638, 8043, 102, 6863, 2768, 1527, 1596, 4638, 1333, 1728, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3221, 784, 720, 1333, 1728, 2193, 5636, 4638, 8043, 102, 1527, 1596, 4638, 1184, 2990, 4568, 4307, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3221, 784, 720, 1333, 1728, 2193, 5636, 4638, 8043, 102, 1527, 1596, 1377, 809, 3780, 2689, 1408, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3221, 784, 720, 1333, 1728, 2193, 5636, 4638, 8043, 102, 1527, 1596, 833, 837, 3381, 1408, 8043, 102]\n",
      "input_ids [101, 5439, 2399, 2595, 782, 1527, 1596, 7444, 6206, 3800, 2692, 1525, 763, 7309, 7579, 8043, 102, 5439, 2399, 782, 2533, 749, 1527, 1596, 2418, 3800, 2692, 1525, 763, 8043, 102]\n",
      "input_ids [101, 5439, 2399, 2595, 782, 1527, 1596, 7444, 6206, 3800, 2692, 1525, 763, 7309, 7579, 8043, 102, 2642, 3300, 1527, 1596, 4638, 5439, 2399, 782, 6206, 3800, 2692, 784, 720, 8043, 102]\n",
      "input_ids [101, 5439, 2399, 2595, 782, 1527, 1596, 7444, 6206, 3800, 2692, 1525, 763, 7309, 7579, 8043, 102, 5439, 782, 1527, 1596, 3300, 862, 4568, 4307, 8043, 102]\n",
      "input_ids [101, 5439, 2399, 2595, 782, 1527, 1596, 7444, 6206, 3800, 2692, 1525, 763, 7309, 7579, 8043, 102, 5439, 782, 3300, 1527, 1596, 2582, 720, 1215, 8043, 102]\n",
      "input_ids [101, 5439, 2399, 2595, 782, 1527, 1596, 7444, 6206, 3800, 2692, 1525, 763, 7309, 7579, 8043, 102, 5439, 2399, 782, 1527, 1596, 6206, 2582, 720, 7564, 7344, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6206, 3800, 2692, 763, 784, 720, 8043, 8043, 8043, 6468, 6468, 102, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6206, 3800, 2692, 1525, 763, 7309, 7579, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6206, 3800, 2692, 763, 784, 720, 8043, 8043, 8043, 6468, 6468, 102, 2533, 749, 6814, 3130, 2595, 1527, 1596, 3300, 784, 720, 7444, 6206, 3800, 2692, 4638, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6206, 3800, 2692, 763, 784, 720, 8043, 8043, 8043, 6468, 6468, 102, 2533, 749, 6814, 3130, 2595, 1527, 1596, 3300, 784, 720, 4568, 4307, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6206, 3800, 2692, 763, 784, 720, 8043, 8043, 8043, 6468, 6468, 102, 6814, 3130, 2595, 1527, 1596, 4567, 4638, 4567, 1728, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 2533, 749, 6814, 3130, 2595, 1527, 1596, 6206, 3800, 2692, 763, 784, 720, 8043, 8043, 8043, 6468, 6468, 102, 6814, 3130, 2595, 1527, 1596, 4567, 4638, 3780, 4545, 3175, 3791, 3300, 784, 720, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 2582, 720, 2218, 4960, 4197, 7313, 1962, 749, 8043, 102, 711, 784, 720, 1527, 1596, 4960, 4197, 2218, 1962, 749, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 2582, 720, 2218, 4960, 4197, 7313, 1962, 749, 8043, 102, 1527, 1596, 4960, 4197, 7313, 1962, 749, 3221, 784, 720, 1333, 1728, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 2582, 720, 2218, 4960, 4197, 7313, 1962, 749, 8043, 102, 1527, 1596, 679, 5543, 5632, 2689, 1408, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 2582, 720, 2218, 4960, 4197, 7313, 1962, 749, 8043, 102, 1527, 1596, 4567, 3221, 2582, 720, 2533, 677, 4638, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 2582, 720, 2218, 4960, 4197, 7313, 1962, 749, 8043, 102, 1527, 1596, 4567, 3300, 1525, 763, 4568, 4307, 8043, 102]\n",
      "input_ids [101, 3300, 1527, 1596, 8024, 6844, 2139, 1343, 978, 6716, 2791, 7248, 4159, 1408, 102, 3300, 1527, 1596, 8024, 5543, 1415, 1343, 978, 6716, 2791, 7248, 4159, 8043, 102]\n",
      "input_ids [101, 3300, 1527, 1596, 8024, 6844, 2139, 1343, 978, 6716, 2791, 7248, 4159, 1408, 102, 3300, 1527, 1596, 6820, 1377, 809, 1343, 978, 6716, 2791, 7248, 4159, 1408, 8043, 102]\n",
      "input_ids [101, 3300, 1527, 1596, 8024, 6844, 2139, 1343, 978, 6716, 2791, 7248, 4159, 1408, 102, 1527, 1596, 4567, 2642, 5442, 6206, 3800, 2692, 784, 720, 752, 7555, 8043, 102]\n",
      "input_ids [101, 3300, 1527, 1596, 8024, 6844, 2139, 1343, 978, 6716, 2791, 7248, 4159, 1408, 102, 3780, 4545, 1527, 1596, 4567, 4638, 3175, 3791, 3300, 1525, 763, 8043, 102]\n",
      "input_ids [101, 3300, 1527, 1596, 8024, 6844, 2139, 1343, 978, 6716, 2791, 7248, 4159, 1408, 102, 1527, 1596, 4567, 4567, 1728, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2418, 3800, 2692, 4638, 7309, 7579, 3300, 784, 720, 8043, 102, 6814, 3130, 2595, 1527, 1596, 2418, 3800, 2692, 1525, 763, 7309, 7579, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2418, 3800, 2692, 4638, 7309, 7579, 3300, 784, 720, 8043, 102, 6814, 3130, 2595, 1527, 1596, 6206, 3800, 2692, 784, 720, 752, 7555, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2418, 3800, 2692, 4638, 7309, 7579, 3300, 784, 720, 8043, 102, 6814, 3130, 2595, 1527, 1596, 3300, 1525, 763, 4568, 4307, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2418, 3800, 2692, 4638, 7309, 7579, 3300, 784, 720, 8043, 102, 784, 720, 3221, 6814, 3130, 2595, 1527, 1596, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2418, 3800, 2692, 4638, 7309, 7579, 3300, 784, 720, 8043, 102, 6814, 3130, 2595, 1527, 1596, 3221, 784, 720, 2471, 6629, 4638, 8043, 102]\n",
      "input_ids [101, 2533, 749, 1527, 1596, 4567, 6820, 1377, 809, 6651, 3635, 7248, 4159, 1408, 102, 1527, 1596, 4567, 2642, 5442, 6820, 5543, 6651, 3635, 7248, 4159, 1408, 8043, 102]\n",
      "input_ids [101, 2533, 749, 1527, 1596, 4567, 6820, 1377, 809, 6651, 3635, 7248, 4159, 1408, 102, 1527, 1596, 4567, 2642, 5442, 5543, 1415, 6651, 3635, 6817, 1220, 8043, 102]\n",
      "input_ids [101, 2533, 749, 1527, 1596, 4567, 6820, 1377, 809, 6651, 3635, 7248, 4159, 1408, 102, 1527, 1596, 4567, 3300, 2533, 3780, 1408, 8043, 102]\n",
      "input_ids [101, 2533, 749, 1527, 1596, 4567, 6820, 1377, 809, 6651, 3635, 7248, 4159, 1408, 102, 1527, 1596, 4567, 3241, 3309, 4568, 4307, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 2533, 749, 1527, 1596, 4567, 6820, 1377, 809, 6651, 3635, 7248, 4159, 1408, 102, 152, 13139, 1527, 1596, 4567, 782, 1377, 809, 7443, 1265, 4500, 1408, 8043, 102]\n",
      "input_ids [101, 7270, 3309, 4638, 1495, 1644, 3221, 2582, 720, 1726, 752, 8024, 671, 2137, 3221, 1527, 1596, 1408, 8043, 102, 711, 784, 720, 7270, 3309, 1495, 1644, 8024, 5507, 2137, 3221, 1527, 1596, 1408, 8043, 102]\n",
      "input_ids [101, 7270, 3309, 4638, 1495, 1644, 3221, 2582, 720, 1726, 752, 8024, 671, 2137, 3221, 1527, 1596, 1408, 8043, 102, 7270, 3309, 4638, 1495, 1644, 3221, 784, 720, 2658, 1105, 8024, 2553, 4197, 3221, 1527, 1596, 1408, 8043, 102]\n",
      "input_ids [101, 7270, 3309, 4638, 1495, 1644, 3221, 2582, 720, 1726, 752, 8024, 671, 2137, 3221, 1527, 1596, 1408, 8043, 102, 1527, 1596, 1495, 1644, 2582, 720, 1215, 8043, 102]\n",
      "input_ids [101, 7270, 3309, 4638, 1495, 1644, 3221, 2582, 720, 1726, 752, 8024, 671, 2137, 3221, 1527, 1596, 1408, 8043, 102, 1527, 1596, 4567, 3221, 2582, 720, 2533, 677, 4638, 8043, 102]\n",
      "input_ids [101, 7270, 3309, 4638, 1495, 1644, 3221, 2582, 720, 1726, 752, 8024, 671, 2137, 3221, 1527, 1596, 1408, 8043, 102, 1963, 862, 7564, 7344, 1527, 1596, 8043, 102]\n",
      "input_ids [101, 3297, 6818, 5541, 7315, 1495, 1644, 1461, 1429, 1737, 7410, 3221, 679, 3221, 2533, 749, 1527, 1596, 8043, 102, 3297, 6818, 5541, 7315, 1495, 1644, 1461, 1429, 1737, 7410, 3221, 679, 3221, 1527, 1596, 4638, 4568, 4307, 8043, 102]\n",
      "input_ids [101, 3297, 6818, 5541, 7315, 1495, 1644, 1461, 1429, 1737, 7410, 3221, 679, 3221, 2533, 749, 1527, 1596, 8043, 102, 3297, 6818, 5541, 7315, 1495, 1644, 1461, 1429, 1737, 7410, 3221, 2533, 749, 1527, 1596, 1408, 8043, 102]\n",
      "input_ids [101, 3297, 6818, 5541, 7315, 1495, 1644, 1461, 1429, 1737, 7410, 3221, 679, 3221, 2533, 749, 1527, 1596, 8043, 102, 1527, 1596, 3300, 1525, 763, 4568, 4307, 6134, 4385, 8043, 102]\n",
      "input_ids [101, 3297, 6818, 5541, 7315, 1495, 1644, 1461, 1429, 1737, 7410, 3221, 679, 3221, 2533, 749, 1527, 1596, 8043, 102, 3297, 6818, 5541, 7315, 1495, 1644, 1461, 1429, 1737, 7410, 3221, 784, 720, 1333, 1728, 8043, 102]\n",
      "input_ids [101, 3297, 6818, 5541, 7315, 1495, 1644, 1461, 1429, 1737, 7410, 3221, 679, 3221, 2533, 749, 1527, 1596, 8043, 102, 1527, 1596, 4567, 4567, 1728, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 1495, 1644, 1391, 784, 720, 3717, 3362, 1962, 8043, 102, 1527, 1596, 1495, 1644, 6844, 2139, 1391, 784, 720, 3717, 3362, 3126, 3362, 1962, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 1495, 1644, 1391, 784, 720, 3717, 3362, 1962, 8043, 102, 1527, 1596, 1495, 1644, 1391, 784, 720, 3717, 3362, 3683, 6772, 1962, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 1495, 1644, 1391, 784, 720, 3717, 3362, 1962, 8043, 102, 1527, 1596, 1495, 1644, 5543, 2582, 720, 7608, 4545, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 1495, 1644, 1391, 784, 720, 3717, 3362, 1962, 8043, 102, 1527, 1596, 1495, 1644, 6206, 857, 7368, 1408, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 1495, 1644, 1391, 784, 720, 3717, 3362, 1962, 8043, 102, 1527, 1596, 1495, 1644, 6421, 2582, 720, 3780, 4545, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3780, 1962, 749, 8024, 711, 784, 720, 2600, 3221, 5541, 7315, 4638, 2523, 102, 1527, 1596, 3780, 1962, 749, 8024, 1316, 2600, 3221, 5541, 7315, 4638, 2523, 3221, 2582, 720, 1726, 752, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3780, 1962, 749, 8024, 711, 784, 720, 2600, 3221, 5541, 7315, 4638, 2523, 102, 1527, 1596, 3780, 1962, 749, 8024, 2600, 3221, 5541, 7315, 4638, 2523, 3221, 784, 720, 1333, 1728, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3780, 1962, 749, 8024, 711, 784, 720, 2600, 3221, 5541, 7315, 4638, 2523, 102, 1527, 1596, 3780, 1962, 749, 833, 1908, 1355, 1408, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3780, 1962, 749, 8024, 711, 784, 720, 2600, 3221, 5541, 7315, 4638, 2523, 102, 1527, 1596, 2582, 720, 3780, 8043, 102]\n",
      "input_ids [101, 1527, 1596, 3780, 1962, 749, 8024, 711, 784, 720, 2600, 3221, 5541, 7315, 4638, 2523, 102, 1527, 1596, 5543, 1415, 3418, 3780, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2582, 720, 3780, 3683, 6772, 1962, 8043, 102, 6814, 3130, 2595, 1527, 1596, 1963, 862, 3780, 3126, 3362, 1962, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2582, 720, 3780, 3683, 6772, 1962, 8043, 102, 2582, 720, 3780, 6814, 3130, 2595, 1527, 1596, 3683, 6772, 1962, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2582, 720, 3780, 3683, 6772, 1962, 8043, 102, 784, 720, 3221, 6814, 3130, 2595, 1527, 1596, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2582, 720, 3780, 3683, 6772, 1962, 8043, 102, 6814, 3130, 2595, 1527, 1596, 3300, 784, 720, 4568, 4307, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 2582, 720, 3780, 3683, 6772, 1962, 8043, 102, 6814, 3130, 2595, 1527, 1596, 6206, 3800, 2692, 784, 720, 8043, 102]\n",
      "input_ids [101, 3780, 1527, 1596, 1168, 1266, 776, 2548, 5526, 7305, 704, 1278, 7368, 2582, 720, 3416, 102, 1266, 776, 2548, 5526, 7305, 704, 1278, 7368, 3780, 4545, 1527, 1596, 4567, 3126, 3362, 1963, 862, 8043, 102]\n",
      "input_ids [101, 3780, 1527, 1596, 1168, 1266, 776, 2548, 5526, 7305, 704, 1278, 7368, 2582, 720, 3416, 102, 1266, 776, 2548, 5526, 7305, 704, 1278, 7368, 3780, 4545, 1527, 1596, 4567, 1962, 679, 1962, 8043, 102]\n",
      "input_ids [101, 3780, 1527, 1596, 1168, 1266, 776, 2548, 5526, 7305, 704, 1278, 7368, 2582, 720, 3416, 102, 1963, 862, 3780, 4545, 1527, 1596, 4567, 8043, 102]\n",
      "input_ids [101, 3780, 1527, 1596, 1168, 1266, 776, 2548, 5526, 7305, 704, 1278, 7368, 2582, 720, 3416, 102, 1527, 1596, 4567, 782, 2398, 3198, 2418, 3800, 2692, 784, 720, 1962, 8043, 102]\n",
      "input_ids [101, 3780, 1527, 1596, 1168, 1266, 776, 2548, 5526, 7305, 704, 1278, 7368, 2582, 720, 3416, 102, 1527, 1596, 4567, 6206, 857, 7368, 1408, 8043, 102]\n",
      "input_ids [101, 1266, 776, 3780, 4545, 1527, 1596, 1962, 4638, 1278, 7368, 3221, 1525, 2157, 102, 1266, 776, 1525, 2157, 1278, 7368, 3780, 4545, 1527, 1596, 1962, 8043, 102]\n",
      "input_ids [101, 1266, 776, 3780, 4545, 1527, 1596, 1962, 4638, 1278, 7368, 3221, 1525, 2157, 102, 1266, 776, 1525, 2157, 1278, 7368, 3780, 4545, 1527, 1596, 3126, 3362, 1962, 8043, 102]\n",
      "input_ids [101, 1266, 776, 3780, 4545, 1527, 1596, 1962, 4638, 1278, 7368, 3221, 1525, 2157, 102, 1527, 1596, 1962, 3780, 1408, 8043, 102]\n",
      "input_ids [101, 1266, 776, 3780, 4545, 1527, 1596, 1962, 4638, 1278, 7368, 3221, 1525, 2157, 102, 1525, 702, 1765, 1277, 3780, 4545, 1527, 1596, 7770, 3126, 8043, 102]\n",
      "input_ids [101, 1266, 776, 3780, 4545, 1527, 1596, 1962, 4638, 1278, 7368, 3221, 1525, 2157, 102, 1527, 1596, 4567, 2582, 720, 3780, 4545, 136, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 4638, 3780, 4545, 3175, 3791, 3300, 1525, 763, 136, 102, 6814, 3130, 2595, 1527, 1596, 4638, 3780, 4545, 3175, 3791, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 4638, 3780, 4545, 3175, 3791, 3300, 1525, 763, 136, 102, 6814, 3130, 2595, 1527, 1596, 3300, 1525, 763, 3780, 4545, 3175, 3791, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 4638, 3780, 4545, 3175, 3791, 3300, 1525, 763, 136, 102, 6814, 3130, 2595, 1527, 1596, 1518, 1525, 763, 4568, 4307, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 4638, 3780, 4545, 3175, 3791, 3300, 1525, 763, 136, 102, 6814, 3130, 2595, 1527, 1596, 4567, 4638, 4567, 1728, 3221, 784, 720, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 4638, 3780, 4545, 3175, 3791, 3300, 1525, 763, 136, 102, 6814, 3130, 2595, 1527, 1596, 4567, 1962, 3780, 1408, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 5790, 5052, 4500, 102, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 5790, 3300, 3126, 3362, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 5790, 5052, 4500, 102, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 5790, 3126, 3362, 1962, 4638, 2571, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 5790, 5052, 4500, 102, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 3717, 3362, 1962, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 5790, 5052, 4500, 102, 6814, 3130, 2595, 1527, 1596, 5543, 3780, 1962, 1408, 8043, 102]\n",
      "input_ids [101, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 5790, 5052, 4500, 102, 6814, 3130, 2595, 1527, 1596, 1391, 784, 720, 5790, 1199, 868, 4500, 3297, 2207, 8043, 102]\n"
     ]
    }
   ],
   "source": [
    "file_based_convert_examples_to_features(Examples, label_list = [0,1], max_seq_length=40, tokenizer=tokenizer\n",
    "    , output_file = path + \"preprocess_data/train.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[101, 1196, 4164, 6817, 1220, 1400, 1492, 6117, 117, 3221, 102, 1196, 4164, 6817, 1220, 1400, 1492, 6117, 3221, 102]\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n1\n"
     ]
    }
   ],
   "source": [
    "a = convert_single_example(0, Examples[0], label_list=[0,1], max_seq_length=20, tokenizer = tokenizer)\n",
    "print(a.input_ids)\n",
    "print(a.input_mask)\n",
    "print(a.segment_ids)\n",
    "print(a.label_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_features = {\n",
    "    'input_ids':tf.io.FixedLenFeature([seq_length],tf.int64)\n",
    "    , 'input_mask':tf.io.FixedLenFeature([seq_length],tf.int64)\n",
    "    , 'segment_ids':tf.io.FixedLenFeature([seq_length],tf.int64)\n",
    "    , 'label_ids':tf.io.FixedLenFeature([1],tf.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_record(record, name_to_features):\n",
    "    example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, tf.int32)\n",
    "        example[name] = t\n",
    "    \n",
    "    return {\"input_ids\": example[\"input_ids\"], \"input_mask\":example[\"input_mask\"], \"segment_ids\":example[\"segment_ids\"]}, example['label_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.TFRecordDataset(path + \"preprocess_data/train.tfrecord\")\n",
    "train_ds = train_ds.map(\n",
    "    lambda record: decode_record(record, name_to_features), num_parallel_calls = tf.data.experimental.AUTOTUNE\n",
    "    ).repeat().batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\narray([[ 101, 1196, 4164, ...,    0,    0,    0],\n       [ 101, 1196, 4164, ...,    0,    0,    0],\n       [ 101, 1196, 4164, ...,    0,    0,    0],\n       ...,\n       [ 101, 2769, 1744, ...,    0,    0,    0],\n       [ 101, 1920, 1492, ...,    0,    0,    0],\n       [ 101, 1920, 1492, ...,    0,    0,    0]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(32, 1), dtype=int32, numpy=\narray([[1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for line in train_ds:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义bert模型\n",
    "def classifier_model(num_labels, max_seq_length = None, final_layer_initializer = None, hub_module_url = None, hub_module_trainable = True):\n",
    "    \n",
    "    if final_layer_initializer is not None:\n",
    "        initializer = final_layer_initializer\n",
    "    else:\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
    "\n",
    "    # 输入三个序列\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype = tf.int32, name = 'input_ids')\n",
    "    input_mask = tf.keras.layers.Input(shape = (max_seq_length,), dtype = tf.int32, name = 'input_mask')\n",
    "    input_type_ids = tf.keras.layers.Input(shape = (max_seq_length,), dtype = tf.int32, name = 'segment_ids')\n",
    "    \n",
    "    bert_model = hub.KerasLayer(hub_module_url, trainable=hub_module_trainable)\n",
    "    pooled_output, _ = bert_model([input_word_ids, input_mask, input_type_ids])\n",
    "    output = tf.keras.layers.Dropout(rate = 0.1)(pooled_output)\n",
    "\n",
    "    output = tf.keras.layers.Dense(num_labels, kernel_initializer=initializer, name = 'output')(output)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs = {'input_ids':input_word_ids, 'input_mask':input_mask, 'segment_ids':input_type_ids}, outputs = output\n",
    "        ), bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, core_model = classifier_model(\n",
    "    num_labels=2\n",
    "    , max_seq_length=40\n",
    "    , hub_module_url=path + \"bert_zh_L-12_H-768_A-12_2\"\n",
    "    , hub_module_trainable = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_ids (InputLayer)          [(None, 40)]         0                                            \n__________________________________________________________________________________________________\ninput_mask (InputLayer)         [(None, 40)]         0                                            \n__________________________________________________________________________________________________\nsegment_ids (InputLayer)        [(None, 40)]         0                                            \n__________________________________________________________________________________________________\nkeras_layer (KerasLayer)        [(None, 768), (None, 102267649   input_ids[0][0]                  \n                                                                 input_mask[0][0]                 \n                                                                 segment_ids[0][0]                \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 768)          0           keras_layer[0][0]                \n__________________________________________________________________________________________________\noutput (Dense)                  (None, 2)            1538        dropout[0][0]                    \n==================================================================================================\nTotal params: 102,269,187\nTrainable params: 102,269,186\nNon-trainable params: 1\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    , optimizer = tf.keras.optimizers.Adam(0.00005)\n",
    "    , metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\narray([[ 101, 1196, 4164, ...,    0,    0,    0],\n       [ 101, 1196, 4164, ...,    0,    0,    0],\n       [ 101, 1196, 4164, ...,    0,    0,    0],\n       ...,\n       [ 101, 2769, 1744, ...,    0,    0,    0],\n       [ 101, 1920, 1492, ...,    0,    0,    0],\n       [ 101, 1920, 1492, ...,    0,    0,    0]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(32, 40), dtype=int32, numpy=\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(32, 1), dtype=int32, numpy=\narray([[1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1],\n       [0],\n       [0],\n       [0],\n       [1],\n       [1]], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for line in train_ds:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "274/274 [==============================] - 39s 100ms/step - loss: 0.3503 - accuracy: 0.8623\n",
      "Epoch 2/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.2374 - accuracy: 0.9105\n",
      "Epoch 3/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.2223 - accuracy: 0.9187\n",
      "Epoch 4/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.1720 - accuracy: 0.9396\n",
      "Epoch 5/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.1257 - accuracy: 0.9573\n",
      "Epoch 6/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.1249 - accuracy: 0.9616\n",
      "Epoch 7/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.0909 - accuracy: 0.9705\n",
      "Epoch 8/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.0689 - accuracy: 0.9797\n",
      "Epoch 9/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.0649 - accuracy: 0.9811\n",
      "Epoch 10/10\n",
      "274/274 [==============================] - 27s 100ms/step - loss: 0.0589 - accuracy: 0.9805\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20c87b2370>"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "model.fit(train_ds, epochs = 10, steps_per_epoch = 274)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 945). These functions will not be directly callable after loading.\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: /data/python/tensorflow/data/save_models/bert_version1/assets\n",
      "INFO:tensorflow:Assets written to: /data/python/tensorflow/data/save_models/bert_version1/assets\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(path + \"save_models\"):\n",
    "    os.mkdir(path + \"save_models\")\n",
    "# 保存模型\n",
    "tf.saved_model.save(model, path + \"save_models/bert_version1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模型\n",
    "restored_saved_model = tf.saved_model.load(path + \"save_models/bert_version1\")\n",
    "f = restored_saved_model.signatures[\"serving_default\"]\n",
    "test_sample = convert_single_example(0, Examples[0], label_list = [0,1], max_seq_length = 40, tokenizer= tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.InputFeatures at 0x7f9a00766610>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_voab(vocab_file):\n",
    "#     vocab = collections.OrderdDict()\n",
    "#     index = 0\n",
    "#     with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n",
    "#         while True:\n",
    "#             token = convert_to_unicode(reader.readline())\n",
    "#             if not token:\n",
    "#                 break\n",
    "#             token = token.strip()\n",
    "#             vocab[token] = index\n",
    "#             index+=1\n",
    "#     return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_by_vocab(vocab, items):\n",
    "#     output = []\n",
    "#     for item in items:\n",
    "#         output.append(vocab[item])\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_tokens_to_ids(vocab, tokens):\n",
    "#     return convert_by_vocab(vocab, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_ids_to_tokens(inv_vocab, ids):\n",
    "#     return convert_by_vocab(inv_vocab, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def whitespace_tokenize(text):\n",
    "#     text = text.strip()\n",
    "#     if not text:\n",
    "#         return []\n",
    "#     tokens = text.split()\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FullTokenizer(object):\n",
    "#     def __init__(self, vocab_file, do_lower_case = True, split_on_punc = True):\n",
    "#         self.vocab = load_vocab(vocab_file)\n",
    "#         self.inv_vocab = {v:k for k, v in self.vocab.items()}\n",
    "#         self.basic_tokenizer = BasicTokenizer(do_lower_case = do_lower_case, split_on_punc = split_on_punc)\n",
    "#         self.wordpiece_tokenizer = WordpieceTokenizer(vocab = self.vocab)\n",
    "\n",
    "#     def tokenize(self, text):\n",
    "#         split_tokens = []\n",
    "#         for token in self.basic_tokenizer.tokenize(text):\n",
    "#             for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
    "#                 split_tokens.append(sub_token)\n",
    "#         return split_tokens\n",
    "\n",
    "#     def covert_tokens_to_ids(self, tokens):\n",
    "#         return convert_by_vocab(self.vocab, tokens)\n",
    "\n",
    "#     def convert_ids_to_tokens(self, ids):\n",
    "#         return convert_by_vocab(self.inv_vocab,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BasicTokenizer(object):\n",
    "#     def __init__(self, do_lower_case = True, split_on_punc = True):\n",
    "#         self.do_lower_case = do_lower_case\n",
    "#         self.split_on_punc = split_on_punc\n",
    "\n",
    "#     def tokenize(self, text):\n",
    "#         text = convert_to_unicode(text)\n",
    "#         text = self._clean_text(text)\n",
    "\n",
    "#         text = self._tokenize_chinese_chars(text)\n",
    "        \n",
    "#         orig_tokens = whitespace_tokenize(text)\n",
    "#         split_tokens = []\n",
    "#         for token in orig_tokens:\n",
    "#             if self.do_lower_case:\n",
    "#                 token = token.lower()\n",
    "#                 token.self._run_strip_accents(token)\n",
    "#             if self.split_on_punc:\n",
    "#                 split_tokens.extend(self._run_split_on_punc(token))\n",
    "#             else:\n",
    "#                 split_tokens.append(token)\n",
    "#         ouput_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
    "#         return ouput_tokens\n",
    "\n",
    "#     def _run_strip_accents(self, text):\n",
    "#         text = unicodedata.normalize(\"NFD\",text)\n",
    "#         output = []\n",
    "#         for char in text:\n",
    "#             cat = unicodedata.catgory(char)\n",
    "#             if cat == 'Mn':\n",
    "#                 continue\n",
    "#             output.append(char)\n",
    "#         return \"\".join(output)\n",
    "\n",
    "#     def _run_split_on_punc(self, text):\n",
    "#         chars = list(text)\n",
    "#         i = 0\n",
    "#         start_new_word = True\n",
    "#         output = []\n",
    "#         while i < len(chars):\n",
    "#             char = char[i]\n",
    "#             if _is_punctuation(char):\n",
    "#                 output.append([char])\n",
    "#                 start_new_word = True\n",
    "#             else:\n",
    "#                 if start_new_word:\n",
    "#                     output.append([])\n",
    "#                 start_new_word =False\n",
    "#                 output[-1].append(char)\n",
    "#             i+=1\n",
    "#         return [\"\".join(x) for x in output]\n",
    "\n",
    "#     def _tokenize_chinese_chars(self, text):\n",
    "#         output = []\n",
    "#         for char in text:\n",
    "#             cp = ord(char)\n",
    "#             if self._is_chinese_char(cp):\n",
    "#                 output.append(\" \")\n",
    "#                 output.append(char)\n",
    "#                 output.append(\" \")\n",
    "#             else:\n",
    "#                 output.append(char)\n",
    "#         return \"\".join(output)\n",
    "\n",
    "#     def _is_chinese_char(self, cp):\n",
    "#         if ((cp >= 0x4E00 and cp <= 0x9FFF) or \n",
    "#         (cp >= 0x3400 and cp <= 0x4DBF) or \n",
    "#         (cp >= 0x20000 and cp <= 0x2A6DF) or \n",
    "#         (cp >= 0x2A700 and cp <= 0x2B73F) or \n",
    "#         (cp >= 0x2B740 and cp <= 0x2B81F) or \n",
    "#         (cp >= 0x2B820 and cp <= 0x2CEAF) or \n",
    "#         (cp >= 0xF900 and cp <= 0xFAFF) or \n",
    "#         (cp >= 0x2F800 and cp <= 0x2FA1F)):\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "#     def _clean_text(self, text):\n",
    "#         output = []\n",
    "#         for char in text:\n",
    "#             cp = ord(char)\n",
    "#             if cp == 0 or cp == 0xfffd or _is_control(char):\n",
    "#                 continue\n",
    "#             if _is_whitespace(char):\n",
    "#                 output.append(\" \")\n",
    "#             else:\n",
    "#                 output.append(char)\n",
    "#         return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WordpieceTokenizer(object):\n",
    "#     def __init__(self, vocab, unk_token = \"[UNK]\", max_input_chars_per_word = 400):\n",
    "#         self.vocab = vocab\n",
    "#         self.unk_token = unk_token\n",
    "#         self.max_input_chars_per_word = max_input_chars_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd008bab98f15494fd332d1f9906437e81d1fefa5db7a806b2babe07711a022cbf0",
   "display_name": "Python 3.8.5 64-bit ('venv38': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "08bab98f15494fd332d1f9906437e81d1fefa5db7a806b2babe07711a022cbf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}