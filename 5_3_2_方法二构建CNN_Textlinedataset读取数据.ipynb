{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import glob\n",
    "import cv2 # 专用于处理计算机视觉场景文件的包 pip install opencv-python 执行安装\n",
    "import json\n",
    "import time\n",
    "# 查询系统可用的 GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# 确保有可用的 GPU 如果没有, 则会报错\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# 设置参数,该段务必在运行jupyter的第一段代码执行，否则会无法初始化成功\n",
    "# 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DP_DIR = '/data/python/tensorflow/shuffle_data_gzip/'\n",
    "BASE_SIZE = 256\n",
    "NCSVS = 100\n",
    "NCATS = 340\n",
    "STEPS = 800\n",
    "EPOCHS = 16\n",
    "size = 64\n",
    "batchsize = 680\n",
    "np.random.seed(seed = 1987)\n",
    "\n",
    "\n",
    "# def f2cat(filename :str) -> str:\n",
    "#     return filename.split('.')[0]\n",
    "\n",
    "# def list_all_categories():\n",
    "#     files = os.listdir('/data/python/tensorflow/shuffle_data_gzip/')\n",
    "#     return sorted([f2cat(f) for f in files], key = str.lower)\n",
    "\n",
    "# def preds2catids(predictions):\n",
    "#     return pd.DataFrame(np.argsort(-predictions, axis = 1)[:, :3], columns = ['a','b','c'])\n",
    "\n",
    "# def top_3_accuracy(y_true, y_pred):\n",
    "#     return tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(line):\n",
    "    column_default = [\n",
    "        tf.constant('0', dtype = tf.string),\n",
    "        tf.constant(0, dtype = tf.int32)\n",
    "    ]\n",
    "    columns = tf.io.decode_csv(line, column_default, select_cols = [1,5])\n",
    "    features = columns[0]\n",
    "    label = columns[1]\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cv2(raw_strokes, size = 64, lw = 6):\n",
    "    raw_strokes = eval(raw_strokes.numpy())\n",
    "    img = np.zeros((256,256), np.uint8)\n",
    "    for stroke in raw_strokes:\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            _ = cv2.line(img, (stroke[0][i],stroke[1][i]),\n",
    "                (stroke[0][i+1],stroke[1][i+1]), 255, lw\n",
    "            )\n",
    "    return cv2.resize(img, (size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_draw_cv2(image, label):\n",
    "    [image] = tf.py_function(draw_cv2, [image], [tf.float32])\n",
    "    image = tf.reshape(image, (64,64,1))\n",
    "    label = tf.one_hot(label, depth = NCATS)\n",
    "    image.set_shape((64,64,1))\n",
    "    label.set_shape((340,))\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = glob.glob('/data/python/tensorflow/shuffle_data_gzip/*.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/python/tensorflow/shuffle_data_gzip/train_k22.csv.gz'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(fileList[:-1])\n",
    "\n",
    "train_ds = train_ds.interleave(\n",
    "        lambda x: tf.data.TextLineDataset(x, compression_type= 'GZIP').skip(1).map(parse_csv,num_parallel_calls = tf.data.experimental.AUTOTUNE),\n",
    "        cycle_length = 4, block_length = 16, num_parallel_calls = tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "\n",
    "# 返回tensor\n",
    "train_ds = train_ds.map(tf_draw_cv2, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE).shuffle(3000).batch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetModel(tf.keras.models.Model):\n",
    "    def __init__(self, size, n_labels, **kwargs):\n",
    "        super(MobileNetModel, self).__init__(**kwargs)\n",
    "        self.base_model = tf.keras.applications.MobileNet(\n",
    "                input_shape = (size, size, 1),\n",
    "                include_top = False,\n",
    "                weights=None,\n",
    "                classes = n_labels\n",
    "            )\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense = tf.keras.layers.Dense(1024, activation='relu')\n",
    "        self.outputs = tf.keras.layers.Dense(n_labels, activation='softmax')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        output_ = self.outputs(x)\n",
    "        return output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MobileNetModel(size = 64, n_labels = NCATS)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "learning_rate = 0.002\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate= learning_rate)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\n",
    "train_top3_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k =3 , name = 'train_top_3_categorical_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy('test_accuracy')\n",
    "test_top3_accuracy = tf.keras.metrics.TopKCategoricalAccuracy(k =3 , name = 'test_top_3_categorical_accuracy')\n",
    "\n",
    "def train_one_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "    train_top3_accuracy(labels, predictions)\n",
    "\n",
    "def val_one_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "    test_top3_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 64, 64, 1) (1024, 340)\n"
     ]
    }
   ],
   "source": [
    "for a, b in train_ds.take(1):\n",
    "    print(a.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = tf.data.TextLineDataset(fileList[-1], compression_type= 'GZIP').skip(1).map(parse_csv,num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.map(tf_draw_cv2, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.batch(1024)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    # 在下一个epoch开始的时候，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_top3_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    test_top3_accuracy.reset_states()\n",
    "\n",
    "    for step,(images, labels) in enumerate(train_ds):\n",
    "        train_one_step(images, labels)\n",
    "\n",
    "        if step %200 == 0:\n",
    "             print('step :{0}, Samples: {1}, Train Loss: {2}, Train Accuracy: {3}, Train Top3 Accuracy: {4}'.format(\n",
    "                 step, (step + 1) * 1024, train_loss.result(), train_accuracy.result() * 100,\n",
    "                 train_top3_accuracy.result() * 100\n",
    "             ))\n",
    "\n",
    "        if step > 1000:\n",
    "            break\n",
    "\n",
    "        \n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch + 1,\n",
    "        train_loss.result(),\n",
    "        train_accuracy.result() * 100,\n",
    "        train_top3_accuracy.result() * 100,\n",
    "        test_loss.result(),\n",
    "        test_accuracy.result() * 100,\n",
    "        test_top3_accuracy.result() * 100\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 总结处理数据的两个关键步骤\n",
    "- 通过parse_csv 解析csv数据\n",
    "- 通过draw_cv2 转换为图片数据"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitcb11fcd8cdb441e4bf52994f94e346da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
