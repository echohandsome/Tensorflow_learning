{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "# 查询系统可用的 GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# 确保有可用的 GPU 如果没有, 则会报错\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# 设置参数,该段务必在运行jupyter的第一段代码执行，否则会无法初始化成功\n",
    "# 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.TFRecord简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFRecord是Google官方推荐的一种数据格式，是Google专门为TensorFlow设计的一种数据格式。\n",
    "\n",
    "实际上，TFRecord是一种二进制文件，其能更好的利用内存，其内部包含了多个**tf.train.Example**，而Example是protocol buffer数据标准的实现，在一个Example消息体中包含了一系列的tf.train.feature属性，而每一个feature是一个key-value的键值对，其中，key是string类型，而value的取值有三种：\n",
    "\n",
    "- bytes_list:可以存储string 和byte两种数据类型\n",
    "- float_list:可以存储float(float32) 和 double(float64)两种数据类型。\n",
    "- int64_list:可以存储：bool,enum,int32,uint32,int64,uint64\n",
    "\n",
    "- int64_list：tf.train.Feature(int64_list = tf.train.int64List(value = 输入))\n",
    "\n",
    "- float_list:tf.train.Feature(float_list = tf.train.FloatList(value = 输入))\n",
    "\n",
    "- bytes_list:tf.train.Feature(bytes_list = tf.train.BytesList(value = 输入))\n",
    "\n",
    "注意：输入必须是list（向量）\n",
    "\n",
    "值得一提的是，TensorFlow源码中到处可见.proto的文件，且这些文件定义了TensorFlow重要的数据结构部分，且多重语言可以直接使用这类数据，很强大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 为什么用TFRcord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFRcord并非是TensorFlow唯一支持的数据格式，你也可以使用CSV或文本等格式，但是对于TensorFlow来说，TFRcord是最友好的，也是最方便的。前面提到，TFRcord内部是一系列实现了protocol buffer数据标准的Example。对于大型数据，像比其余数据格式，protocol buffer类型的数据优势很明显。_\n",
    "\n",
    "在数据集较小时，我们会把数据全部加载到内存里方便快速导入，但当数据量超过内存大小时，就只能放在硬盘上来一点点读取，这时就不得不考虑数据的移动、读取、处理等速度。使用TFRecord就是为了提速和节约空间的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TFRcord格式理解](./markdown_pics/TFRcord格式理解.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TFRcord格式理解](./markdown_pics/TFRcord格式理解2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.写入TFRecord文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![TFRcord格式理解](./markdown_pics/生成TFRecord格式数据.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.读取TFRecord文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![读取TFRecord格式数据](./markdown_pics/读取TFRecord格式数据.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.案例详解\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/hp/git/learningzone/huangpei/eat_tensorflow2_in_30_days/')\n",
    "data_dir = './data/datasets'\n",
    "train_cats_dir = data_dir + '/train/cats/'\n",
    "train_dogs_dir = data_dir + '/train/dogs/'\n",
    "test_cats_dir = data_dir + '/valid/cats/'\n",
    "test_dogs_dir = data_dir + '/valid/dogs/' \n",
    "\n",
    "train_tfrecord_file = data_dir + '/train/train.tfrecords'\n",
    "test_tfrecord_file = data_dir + '/valid/test.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 对训练集执行读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_filenames = [train_cats_dir + filename for filename in os.listdir(train_cats_dir)]\n",
    "\n",
    "train_dog_filenames = [train_dogs_dir + filename for filename in os.listdir(train_dogs_dir)]\n",
    "\n",
    "train_filenames = train_cat_filenames + train_dog_filenames\n",
    "\n",
    "\n",
    "# 对数据集配置标签 cat 0 dog 1\n",
    "train_labels = [0] * len(train_cat_filenames) + [1] * len(train_dog_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(train_tfrecord_file) as writer:\n",
    "    for filename, label in zip(train_filenames, train_labels):\n",
    "\n",
    "        image = open(filename, 'rb').read()  # 读取数据集图片到内存， image为一个byte类型的字符串\n",
    "\n",
    "        feature = {   # 建立一个tf.train.Feature的字典\n",
    "            'image':tf.train.Feature(bytes_list = tf.train.BytesList(value = [image])), # 图片是一个Bytes对象\n",
    "            'label':tf.train.Feature(int64_list = tf.train.Int64List(value = [label]))  # 标签是一个Int对象\n",
    "        }\n",
    "        example = tf.train.Example(features = tf.train.Features(feature = feature))  # 通过字典建立Example\n",
    "\n",
    "        writer.write(example.SerializePartialToString())  # 将Example 序列化并写入TFrecord文件  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 对验证集执行读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cat_filenames = [test_cats_dir + filename for filename in os.listdir(test_cats_dir)]\n",
    "\n",
    "test_dog_filenames = [test_dogs_dir + filename for filename in os.listdir(test_dogs_dir)]\n",
    "\n",
    "test_filenames = test_cat_filenames + test_dog_filenames\n",
    "\n",
    "\n",
    "# 对数据集配置标签 cat 0 dog 1\n",
    "test_labels = [0] * len(test_cat_filenames) + [1] * len(test_dog_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(test_tfrecord_file) as writer:\n",
    "    for filename, label in zip(test_filenames, test_labels):\n",
    "\n",
    "        image = open(filename, 'rb').read()  # 读取数据集图片到内存， image为一个byte类型的字符串\n",
    "\n",
    "        feature = {   # 建立一个tf.test.Feature的字典\n",
    "            'image':tf.train.Feature(bytes_list = tf.train.BytesList(value = [image])), # 图片是一个Bytes对象\n",
    "            'label':tf.train.Feature(int64_list = tf.train.Int64List(value = [label]))  # 标签是一个Int对象\n",
    "        }\n",
    "        example = tf.train.Example(features = tf.train.Features(feature = feature))  # 通过字典建立Example\n",
    "\n",
    "        serialized = example.SerializePartialToString()  # 将Example序列化\n",
    "        writer.write(serialized)  # 写入TFrecord文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取TFRecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = { # 定义 feature结构，告诉解码器每个Feature的类型是什么\n",
    "    'image':tf.io.FixedLenFeature([], tf.string),\n",
    "    'label':tf.io.FixedLenFeature([], tf.int64),\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_example(example_string): # 将TFRecord 文件中的每一个序列化的 tf.train.Example 解码\n",
    "    feature_dict = tf.io.parse_single_example(example_string, feature_description)\n",
    "\n",
    "    feature_dict['image'] = tf.io.decode_jpeg(feature_dict['image'])  # 解码JPEG图片\n",
    "\n",
    "    feature_dict['image'] = tf.image.resize(feature_dict['image'], [256,256]) /255.0\n",
    "\n",
    "    return feature_dict['image'], feature_dict['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "#### 训练集读取\n",
    "train_dataset = tf.data.TFRecordDataset(train_tfrecord_file) # 读取 TFRecord文件\n",
    "\n",
    "train_dataset = train_dataset.map(_parse_example)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 23000)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "#### 测试集读取\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(test_tfrecord_file)  # 读取TFRecord文件\n",
    "\n",
    "test_dataset = test_dataset.map(_parse_example)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设计神经网络模型,卷积和池化\n",
    "class CNNModel(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(32,3, activation= 'relu')\n",
    "        self.maxpool1 = tf.keras.layers.MaxPooling2D()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32,5,activation='relu')\n",
    "        self.maxpool2 = tf.keras.layers.MaxPooling2D()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.d1 = tf.keras.layers.Dense(64,activation='relu')\n",
    "        self.d2 = tf.keras.layers.Dense(2, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "model = CNNModel()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate= learning_rate)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6903296709060669, Accuracy: 56.017391204833984, Test Loss: 0.644851803779602, Test Accuracy: 63.55000305175781\n",
      "Epoch 2, Loss: 0.6089661121368408, Accuracy: 67.38260650634766, Test Loss: 0.6182286739349365, Test Accuracy: 67.54999542236328\n",
      "Epoch 3, Loss: 0.5100833177566528, Accuracy: 74.71304321289062, Test Loss: 0.5957335233688354, Test Accuracy: 69.19999694824219\n",
      "Epoch 4, Loss: 0.3423081934452057, Accuracy: 84.91304779052734, Test Loss: 0.7056066393852234, Test Accuracy: 68.8499984741211\n",
      "Epoch 5, Loss: 0.13823211193084717, Accuracy: 94.9000015258789, Test Loss: 1.0297844409942627, Test Accuracy: 69.0999984741211\n",
      "Epoch 6, Loss: 0.03975549340248108, Accuracy: 98.81304168701172, Test Loss: 1.6096988916397095, Test Accuracy: 67.75\n",
      "Epoch 7, Loss: 0.01992863230407238, Accuracy: 99.47391510009766, Test Loss: 1.7852365970611572, Test Accuracy: 67.8499984741211\n",
      "Epoch 8, Loss: 0.017125653102993965, Accuracy: 99.55652618408203, Test Loss: 2.081528425216675, Test Accuracy: 68.55000305175781\n",
      "Epoch 9, Loss: 0.022030850872397423, Accuracy: 99.32608795166016, Test Loss: 2.0483222007751465, Test Accuracy: 67.0\n",
      "Epoch 10, Loss: 0.014298616908490658, Accuracy: 99.5999984741211, Test Loss: 2.331361770629883, Test Accuracy: 66.54999542236328\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    # 在下一个epoch开始的时候，重置评估指标\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels) \n",
    "\n",
    "\n",
    "    for test_images, test_labels in test_dataset:\n",
    "        test_step(test_images, test_labels) \n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(\n",
    "        epoch + 1,\n",
    "        train_loss.result(),\n",
    "        train_accuracy.result() * 100,\n",
    "        test_loss.result(),\n",
    "        test_accuracy.result() * 100\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitcb11fcd8cdb441e4bf52994f94e346da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
