{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:45.713353Z",
     "start_time": "2021-02-18T00:30:45.703532Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# 查询系统可用的 GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# 确保有可用的 GPU 如果没有, 则会报错\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# 设置参数,该段务必在运行jupyter的第一段代码执行，否则会无法初始化成功\n",
    "# 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一、神经网络的正向传播和反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自动求导机制\n",
    "- 梯度求导函数：tf.GradientTape\n",
    "- GradientTape是eager模式下计算梯度的函数，TensorFlow2.0默认以eager模式执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:46.420562Z",
     "start_time": "2021-02-18T00:30:46.406244Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "# 举例使用这个函数计算梯度\n",
    "x =tf.constant(3.0) # 定义常数\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x) # watch的作用：确保某个tensor被tape追踪\n",
    "    y = x*x + x**3\n",
    "dy_dx = g.gradient(y,x) #参数里面的y是指刚刚定义的函数，那么x就是对其中哪一个参数求导\n",
    "tf.print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:46.550229Z",
     "start_time": "2021-02-18T00:30:46.539160Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个求导公式并求取梯度值\n",
    "def dy_du(u:float):\n",
    "    u = tf.constant(u)\n",
    "    with tf.GradientTape(persistent=False) as g:\n",
    "        g.watch(u) # watch的作用：确保某个tensor被tape追踪\n",
    "        y = u**2 + u**3 + u**np.exp(1)\n",
    "    dy_du = g.gradient(y,u)\n",
    "    return tf.print(dy_du)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:46.675508Z",
     "start_time": "2021-02-18T00:30:46.665861Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[50.9524307 24.9443512 85.4308777 0 179.071442]\n"
     ]
    }
   ],
   "source": [
    "# 求偏导数\n",
    "dy_du([3.0,2.0,4.0,0,6.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:46.814343Z",
     "start_time": "2021-02-18T00:30:46.799263Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on class GradientTape in module tensorflow.python.eager.backprop:\n\nclass GradientTape(builtins.object)\n |  GradientTape(persistent=False, watch_accessed_variables=True)\n |  \n |  Record operations for automatic differentiation.\n |  \n |  Operations are recorded if they are executed within this context manager and\n |  at least one of their inputs is being \"watched\".\n |  \n |  Trainable variables (created by `tf.Variable` or `tf.compat.v1.get_variable`,\n |  where `trainable=True` is default in both cases) are automatically watched.\n |  Tensors can be manually watched by invoking the `watch` method on this context\n |  manager.\n |  \n |  For example, consider the function `y = x * x`. The gradient at `x = 3.0` can\n |  be computed as:\n |  \n |  ```python\n |  x = tf.constant(3.0)\n |  with tf.GradientTape() as g:\n |    g.watch(x)\n |    y = x * x\n |  dy_dx = g.gradient(y, x) # Will compute to 6.0\n |  ```\n |  \n |  GradientTapes can be nested to compute higher-order derivatives. For example,\n |  \n |  ```python\n |  x = tf.constant(3.0)\n |  with tf.GradientTape() as g:\n |    g.watch(x)\n |    with tf.GradientTape() as gg:\n |      gg.watch(x)\n |      y = x * x\n |    dy_dx = gg.gradient(y, x)     # Will compute to 6.0\n |  d2y_dx2 = g.gradient(dy_dx, x)  # Will compute to 2.0\n |  ```\n |  \n |  By default, the resources held by a GradientTape are released as soon as\n |  GradientTape.gradient() method is called. To compute multiple gradients over\n |  the same computation, create a persistent gradient tape. This allows multiple\n |  calls to the gradient() method as resources are released when the tape object\n |  is garbage collected. For example:\n |  \n |  ```python\n |  x = tf.constant(3.0)\n |  with tf.GradientTape(persistent=True) as g:\n |    g.watch(x)\n |    y = x * x\n |    z = y * y\n |  dz_dx = g.gradient(z, x)  # 108.0 (4*x^3 at x = 3)\n |  dy_dx = g.gradient(y, x)  # 6.0\n |  del g  # Drop the reference to the tape\n |  ```\n |  \n |  By default GradientTape will automatically watch any trainable variables that\n |  are accessed inside the context. If you want fine grained control over which\n |  variables are watched you can disable automatic tracking by passing\n |  `watch_accessed_variables=False` to the tape constructor:\n |  \n |  ```python\n |  with tf.GradientTape(watch_accessed_variables=False) as tape:\n |    tape.watch(variable_a)\n |    y = variable_a ** 2  # Gradients will be available for `variable_a`.\n |    z = variable_b ** 3  # No gradients will be available since `variable_b` is\n |                         # not being watched.\n |  ```\n |  \n |  Note that when using models you should ensure that your variables exist when\n |  using `watch_accessed_variables=False`. Otherwise it's quite easy to make your\n |  first iteration not have any gradients:\n |  \n |  ```python\n |  a = tf.keras.layers.Dense(32)\n |  b = tf.keras.layers.Dense(32)\n |  \n |  with tf.GradientTape(watch_accessed_variables=False) as tape:\n |    tape.watch(a.variables)  # Since `a.build` has not been called at this point\n |                             # `a.variables` will return an empty list and the\n |                             # tape will not be watching anything.\n |    result = b(a(inputs))\n |    tape.gradient(result, a.variables)  # The result of this computation will be\n |                                        # a list of `None`s since a's variables\n |                                        # are not being watched.\n |  ```\n |  \n |  Note that only tensors with real or complex dtypes are differentiable.\n |  \n |  Methods defined here:\n |  \n |  __del__(self)\n |  \n |  __enter__(self)\n |      Enters a context inside which operations are recorded on this tape.\n |  \n |  __exit__(self, typ, value, traceback)\n |      Exits the recording context, no further operations are traced.\n |  \n |  __init__(self, persistent=False, watch_accessed_variables=True)\n |      Creates a new GradientTape.\n |      \n |      Args:\n |        persistent: Boolean controlling whether a persistent gradient tape\n |          is created. False by default, which means at most one call can\n |          be made to the gradient() method on this object.\n |        watch_accessed_variables: Boolean controlling whether the tape will\n |          automatically `watch` any (trainable) variables accessed while the tape\n |          is active. Defaults to True meaning gradients can be requested from any\n |          result computed in the tape derived from reading a trainable `Variable`.\n |          If False users must explicitly `watch` any `Variable`s they want to\n |          request gradients from.\n |  \n |  batch_jacobian(self, target, source, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>, parallel_iterations=None, experimental_use_pfor=True)\n |      Computes and stacks per-example jacobians.\n |      \n |      See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant) for the\n |      definition of a Jacobian. This function is essentially an efficient\n |      implementation of the following:\n |      \n |      `tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.\n |      \n |      Note that compared to `GradientTape.jacobian` which computes gradient of\n |      each output value w.r.t each input value, this function is useful when\n |      `target[i,...]` is independent of `source[j,...]` for `j != i`. This\n |      assumption allows more efficient computation as compared to\n |      `GradientTape.jacobian`. The output, as well as intermediate activations,\n |      are lower dimensional and avoid a bunch of redundant zeros which would\n |      result in the jacobian computation given the independence assumption.\n |      \n |      Example usage:\n |      \n |      ```python\n |      with tf.GradientTape() as g:\n |        x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n |        g.watch(x)\n |        y = x * x\n |      batch_jacobian = g.batch_jacobian(y, x)\n |      # batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n |      ```\n |      \n |      Args:\n |        target: A tensor with rank 2 or higher and with shape [b, y1, ..., y_n].\n |          `target[i,...]` should only depend on `source[i,...]`.\n |        source: A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].\n |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n |          alters the value which will be returned if the target and sources are\n |          unconnected. The possible values and effects are detailed in\n |          'UnconnectedGradients' and it defaults to 'none'.\n |        parallel_iterations: A knob to control how many iterations are dispatched\n |          in parallel. This knob can be used to control the total memory usage.\n |        experimental_use_pfor: If true, uses pfor for computing the Jacobian. Else\n |          uses a tf.while_loop.\n |      \n |      Returns:\n |        A tensor `t` with shape [b, y_1, ..., y_n, x1, ..., x_m] where `t[i, ...]`\n |        is the jacobian of `target[i, ...]` w.r.t. `source[i, ...]`, i.e. stacked\n |        per-example jacobians.\n |      \n |      Raises:\n |        RuntimeError: If called on a non-persistent tape with eager execution\n |          enabled and without enabling experimental_use_pfor.\n |        ValueError: If vectorization of jacobian computation fails or if first\n |          dimension of `target` and `source` do not match.\n |  \n |  gradient(self, target, sources, output_gradients=None, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>)\n |      Computes the gradient using operations recorded in context of this tape.\n |      \n |      Args:\n |        target: a list or nested structure of Tensors or Variables to be\n |          differentiated.\n |        sources: a list or nested structure of Tensors or Variables. `target`\n |          will be differentiated against elements in `sources`.\n |        output_gradients: a list of gradients, one for each element of\n |          target. Defaults to None.\n |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n |          alters the value which will be returned if the target and sources are\n |          unconnected. The possible values and effects are detailed in\n |          'UnconnectedGradients' and it defaults to 'none'.\n |      \n |      Returns:\n |        a list or nested structure of Tensors (or IndexedSlices, or None),\n |        one for each element in `sources`. Returned structure is the same as\n |        the structure of `sources`.\n |      \n |      Raises:\n |        RuntimeError: if called inside the context of the tape, or if called more\n |         than once on a non-persistent tape.\n |        ValueError: if the target is a variable or if unconnected gradients is\n |         called with an unknown value.\n |  \n |  jacobian(self, target, sources, unconnected_gradients=<UnconnectedGradients.NONE: 'none'>, parallel_iterations=None, experimental_use_pfor=True)\n |      Computes the jacobian using operations recorded in context of this tape.\n |      \n |      See [wikipedia article](http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant) for the\n |      definition of a Jacobian.\n |      \n |      Example usage:\n |      \n |      ```python\n |      with tf.GradientTape() as g:\n |        x  = tf.constant([1.0, 2.0])\n |        g.watch(x)\n |        y = x * x\n |      jacobian = g.jacobian(y, x)\n |      # jacobian value is [[2., 0.], [0., 4.]]\n |      ```\n |      \n |      Args:\n |        target: Tensor to be differentiated.\n |        sources: a list or nested structure of Tensors or Variables. `target`\n |          will be differentiated against elements in `sources`.\n |        unconnected_gradients: a value which can either hold 'none' or 'zero' and\n |          alters the value which will be returned if the target and sources are\n |          unconnected. The possible values and effects are detailed in\n |          'UnconnectedGradients' and it defaults to 'none'.\n |        parallel_iterations: A knob to control how many iterations are dispatched\n |          in parallel. This knob can be used to control the total memory usage.\n |        experimental_use_pfor: If true, vectorizes the jacobian computation. Else\n |          falls back to a sequential while_loop. Vectorization can sometimes fail\n |          or lead to excessive memory usage. This option can be used to disable\n |          vectorization in such cases.\n |      \n |      Returns:\n |        A list or nested structure of Tensors (or None), one for each element in\n |        `sources`. Returned structure is the same as the structure of `sources`.\n |        Note if any gradient is sparse (IndexedSlices), jacobian function\n |        currently makes it dense and returns a Tensor instead. This may change in\n |        the future.\n |      \n |      \n |      Raises:\n |        RuntimeError: If called on a non-persistent tape with eager execution\n |          enabled and without enabling experimental_use_pfor.\n |        ValueError: If vectorization of jacobian computation fails.\n |  \n |  reset(self)\n |      Clears all information stored in this tape.\n |      \n |      Equivalent to exiting and reentering the tape context manager with a new\n |      tape. For example, the two following code blocks are equivalent:\n |      \n |      ```\n |      with tf.GradientTape() as t:\n |        loss = loss_fn()\n |      with tf.GradientTape() as t:\n |        loss += other_loss_fn()\n |      t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n |      \n |      \n |      # The following is equivalent to the above\n |      with tf.GradientTape() as t:\n |        loss = loss_fn()\n |        t.reset()\n |        loss += other_loss_fn()\n |      t.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n |      ```\n |      \n |      This is useful if you don't want to exit the context manager for the tape,\n |      or can't because the desired reset point is inside a control flow construct:\n |      \n |      ```\n |      with tf.GradientTape() as t:\n |        loss = ...\n |        if loss > k:\n |          t.reset()\n |      ```\n |  \n |  stop_recording(self)\n |      Temporarily stops recording operations on this tape.\n |      \n |      Operations executed while this context manager is active will not be\n |      recorded on the tape. This is useful for reducing the memory used by tracing\n |      all computations.\n |      \n |      For example:\n |      \n |      ```\n |        with tf.GradientTape(persistent=True) as t:\n |          loss = compute_loss(model)\n |          with t.stop_recording():\n |            # The gradient computation below is not traced, saving memory.\n |            grads = t.gradient(loss, model.variables)\n |      ```\n |      \n |      Yields:\n |        None\n |      Raises:\n |        RuntimeError: if the tape is not currently recording.\n |  \n |  watch(self, tensor)\n |      Ensures that `tensor` is being traced by this tape.\n |      \n |      Args:\n |        tensor: a Tensor or list of Tensors.\n |      \n |      Raises:\n |        ValueError: if it encounters something that is not a tensor.\n |  \n |  watched_variables(self)\n |      Returns variables watched by this tape in order of construction.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(tf.GradientTape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:46.989549Z",
     "start_time": "2021-02-18T00:30:46.970257Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.backprop.GradientTape at 0x7fc9c41846d0>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "tf.GradientTape(persistent=False,watch_accessed_variables=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- persistent:指定创建的GradientTape是否可持续调用。False表示只能调用一次gradient()函数。是否链式求导\n",
    "- watch_accessed_variables：表明这个GradientTape是不是会自动追踪任何能够被训练的变量。如果是False则需要指定追踪哪些变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient(target,sources)\n",
    "##### 根据tape上面的上下文来计算某个或者某些tensor的梯度参数\n",
    "- target 被微分的tensor，可以理解为损失值\n",
    "- sources Tensors 或者变量列表\n",
    "gradient(target,sources)\n",
    "- 返回值：一个列表表示各个变量的梯度值，和sources中的变量列表一一对应，表明这个变量的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结合模型的数据调用tf.GradientTape示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:47.503201Z",
     "start_time": "2021-02-18T00:30:47.498428Z"
    }
   },
   "outputs": [],
   "source": [
    "### 14:43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:47.617910Z",
     "start_time": "2021-02-18T00:30:47.611016Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy() # 定义损失函数\n",
    "optimizer = tf.keras.optimizers.Adam() # 定义优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 正向传播和反向传播的调用示例\n",
    "\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(data)\n",
    "    loss = loss_object(labels,predictions)\n",
    "gradients = tape.gradient(loss,model.trainable_variables) # 定义梯度\n",
    "optimizer.apply_gradients(zip(gradients,model.trainable_variables)) #定义调用\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将计算出来的梯度更新到变量上面去\n",
    "#### apply_gradients(grads_and_vars,name = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### 案例1：实现模型自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型（前向传播）-->定义损失函数-->定义优化函数-->定义tape\n",
    "-->模型得到预测值-->前向传播得到loss-->反向传播-->用优化函数将计算出来的梯度更新到变量上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:48.466304Z",
     "start_time": "2021-02-18T00:30:48.456726Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个类表示前向传播的过程\n",
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,num_classes=10):\n",
    "        super().__init__(name = 'my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32,activation='relu') # 隐藏层\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes) # 输出层\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        # 定义前向传播\n",
    "        # 使用在__init__定义的层\n",
    "        x = self.dense_1(inputs) # 经过第一个隐藏层\n",
    "        return self.dense_2(x) # 返回预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:48.626726Z",
     "start_time": "2021-02-18T00:30:48.613785Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.random.random((1000,32))\n",
    "labels = np.random.random((1000,10)) # 10个类别\n",
    "data = tf.cast(data, tf.float32) # numpy转换为tensor\n",
    "labels = tf.cast(labels, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:48.756031Z",
     "start_time": "2021-02-18T00:30:48.746538Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.22049998, 0.31213003, 0.5519902 , 0.9000708 , 0.16034448,\n",
       "        0.39964816, 0.9113272 , 0.67348826, 0.37222522, 0.08691391],\n",
       "       [0.62849903, 0.30306992, 0.7365023 , 0.04017013, 0.6432007 ,\n",
       "        0.9647105 , 0.02479025, 0.7382544 , 0.42798683, 0.46305203]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "labels[0:2] # 每一个数组的对应位置表示这个数据归属于哪一类的概率较高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:48.896222Z",
     "start_time": "2021-02-18T00:30:48.889046Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.EagerTensor"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:49.036889Z",
     "start_time": "2021-02-18T00:30:49.029964Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on built-in function random:\n\nrandom(...) method of numpy.random.mtrand.RandomState instance\n    random(size=None)\n    \n    Return random floats in the half-open interval [0.0, 1.0). Alias for\n    `random_sample` to ease forward-porting to the new random API.\n\n"
     ]
    }
   ],
   "source": [
    "help(np.random.random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:49.466503Z",
     "start_time": "2021-02-18T00:30:49.179490Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model = MyModel(num_classes=10)\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True) # 定义损失函数\n",
    "optimizer = tf.keras.optimizers.Adam() # 定义优化方法\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    predictions = model(data)\n",
    "    loss = loss_object(labels,predictions)\n",
    "gradients = tape.gradient(loss,model.trainable_variables) # 定义梯度\n",
    "optimizer.apply_gradients(zip(gradients,model.trainable_variables)) #定义调用，将梯度结果更新到变量上去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:49.472733Z",
     "start_time": "2021-02-18T00:30:49.468438Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "type(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:49.478115Z",
     "start_time": "2021-02-18T00:30:49.474525Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "len(model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32, 32)\n(32,)\n(32, 10)\n(10,)\n"
     ]
    }
   ],
   "source": [
    "# 这里包含了权重矩阵和偏置向量，0和2是权重矩阵，1和3是偏置向量\n",
    "print(model.trainable_variables[0].shape)\n",
    "print(model.trainable_variables[1].shape)\n",
    "print(model.trainable_variables[2].shape)\n",
    "print(model.trainable_variables[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 案例2、使用GradientTape自定义训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:50.684077Z",
     "start_time": "2021-02-18T00:30:50.675061Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个类表示前向传播的过程\n",
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(MyModel,self).__init__(name = 'my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32,activation='relu') # 隐藏层\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes) # 输出层\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        # 定义前向传播\n",
    "        # 使用在__init__定义的层\n",
    "        x = self.dense_1(inputs) # 经过第一个隐藏层\n",
    "        return self.dense_2(x) # 返回预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:50.872410Z",
     "start_time": "2021-02-18T00:30:50.864549Z"
    }
   },
   "outputs": [],
   "source": [
    "data = np.random.random((1000,32))\n",
    "labels = np.random.random((1000,10)) # 10个类别\n",
    "# data = tf.cast(data, tf.float32) \n",
    "# labels = tf.cast(labels, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:51.061544Z",
     "start_time": "2021-02-18T00:30:51.041133Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MyModel(num_classes = 10)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:51.849912Z",
     "start_time": "2021-02-18T00:30:51.842572Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 32), (None, 10)), types: (tf.float64, tf.float64)>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:52.707462Z",
     "start_time": "2021-02-18T00:30:52.453242Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start of epoch (0,)\n",
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "training loss for one batch at step 0:\t39.42831039428711\n",
      "seen so far:64 samples\n",
      "start of epoch (1,)\n",
      "training loss for one batch at step 0:\t12.252761840820312\n",
      "seen so far:64 samples\n",
      "start of epoch (2,)\n",
      "training loss for one batch at step 0:\t11.841941833496094\n",
      "seen so far:64 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f'start of epoch {(epoch,)}')\n",
    "    \n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training = True) # 执行前向传播\n",
    "            \n",
    "            loss_value = loss_fn(y_batch_train, logits) # 计算每一轮迭代的损失\n",
    "            \n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        \n",
    "        # 通过梯度更新参数\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(f\"training loss for one batch at step {step}:\\t{float(loss_value)}\")\n",
    "            print(f\"seen so far:{(step+1)*64} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-17T07:18:43.159336Z",
     "start_time": "2021-02-17T07:18:43.149282Z"
    }
   },
   "source": [
    "##### 案例3： 使用GradientTape自定义训练模型进阶（加入评估函数）\n",
    "让我们将 metric添加到组合中，下面可以从头开始编写的训练循环中随时使用内置指标（或编写自定义指标）。流程如下：\n",
    "- 再循环开始时初始化metric\n",
    "- metric.update_state():每个batch之后更新\n",
    "- metric.result():需要显示metric的当前值时调用\n",
    "- metric.reset_states():需要清除metric状态时重置（通常在每个epoch的结尾）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:27.301587Z",
     "start_time": "2021-02-18T00:30:21.255Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个类表示前向传播的过程\n",
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(MyModel,self).__init__(name = 'my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # 定义自己需要的层\n",
    "        self.dense_1 = tf.keras.layers.Dense(32,activation='relu') # 隐藏层\n",
    "        self.dense_2 = tf.keras.layers.Dense(num_classes) # 输出层\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        # 定义前向传播\n",
    "        # 使用在__init__定义的层\n",
    "        x = self.dense_1(inputs) # 经过第一个隐藏层\n",
    "        return self.dense_2(x) # 返回预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:27.302321Z",
     "start_time": "2021-02-18T00:30:21.257Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.random.random((1000,32))\n",
    "y_train = np.random.random((1000,10))\n",
    "\n",
    "x_val = np.random.random((200,32))\n",
    "y_val = np.random.random((200,10))\n",
    "\n",
    "x_test = np.random.random((200,32))\n",
    "y_test = np.random.random((200,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:27.303150Z",
     "start_time": "2021-02-18T00:30:21.259Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:27.303875Z",
     "start_time": "2021-02-18T00:30:21.260Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 1024).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val,y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:27.304532Z",
     "start_time": "2021-02-18T00:30:21.261Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MyModel(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:30:27.305235Z",
     "start_time": "2021-02-18T00:30:21.263Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start of epoch (0,)\n",
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Training acc over epoch{(float(train_acc),)}\n",
      "val acc (0.10499999672174454,)\n",
      "start of epoch (1,)\n",
      "Training acc over epoch{(float(train_acc),)}\n",
      "val acc (0.10000000149011612,)\n",
      "start of epoch (2,)\n",
      "Training acc over epoch{(float(train_acc),)}\n",
      "val acc (0.0949999988079071,)\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f'start of epoch {(epoch,)}')\n",
    "    \n",
    "    # 上文已经定义好了batch_size大小是64\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        \n",
    "        # 先定义跟踪梯度的结构\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training = True) # 预测值\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # 对每一个变量求梯度\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        \n",
    "        # 通过梯度反向传播更新参数\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        \n",
    "        # 完成反向传播以后评估预测效果\n",
    "        train_acc_metric(y_batch_train,logits)\n",
    "    \n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(\"Training acc over epoch{(float(train_acc),)}\")\n",
    "    train_acc_metric.reset_states()\n",
    "        \n",
    "#         if step % 200 == 0:\n",
    "#             print(f\"training loss for one batch at step {step}:\\t{float(loss_value)}\")\n",
    "#             print(f\"seen so far:{(step+1)*64} samples\")\n",
    "\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val)\n",
    "        val_acc_metric(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    print(f\"val acc {(float(val_acc),)}\")\n",
    "    val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-18T00:39:22.101194Z",
     "start_time": "2021-02-18T00:39:22.096057Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function result in module tensorflow.python.keras.metrics:\n\nresult(self)\n    Computes and returns the metric value tensor.\n    \n    Result computation is an idempotent operation that simply calculates the\n    metric value using the state variables.\n\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.metrics.CategoricalAccuracy.result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitcb11fcd8cdb441e4bf52994f94e346da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}