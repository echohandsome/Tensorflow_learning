{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bit03daf4fe1261446982afba36487b0314",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import jieba  # 中文分词库，百度员工开发\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "# 查询系统可用的 GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# 确保有可用的 GPU 如果没有, 则会报错\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# 设置参数,该段务必在运行jupyter的第一段代码执行，否则会无法初始化成功\n",
    "# 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "source": [
    "#### 1.通过IMDB理解文本如何做one_hot编码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "# 定义数据的本地导入\n",
    "path_tmp = '/home/hp/.local/lib/python3.8/site-packages/tensorflow/keras/datasets/'\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(path=path_tmp+'imdb.npz',\n",
    "num_words=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "list"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "218\n189\n141\n550\n147\n"
    }
   ],
   "source": [
    "# 查看训练数据前五个的数组长度\n",
    "for i in range(5):\n",
    "    print(len(train_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
    }
   ],
   "source": [
    "# 查看第一个训练数据\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8, 8, 9, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 21, 22, 22, 22, 22, 22, 22, 25, 25, 25, 25, 26, 26, 26, 28, 28, 30, 32, 32, 32, 33, 33, 35, 36, 36, 36, 36, 38, 38, 38, 38, 39, 43, 43, 43, 43, 46, 48, 50, 50, 51, 51, 52, 56, 62, 65, 65, 66, 66, 71, 71, 76, 77, 82, 87, 88, 88, 92, 98, 100, 103, 104, 104, 106, 107, 112, 112, 113, 117, 124, 130, 134, 135, 141, 144, 147, 150, 167, 172, 172, 173, 178, 192, 194, 215, 224, 226, 226, 256, 256, 283, 284, 297, 316, 317, 336, 381, 385, 386, 400, 407, 447, 458, 469, 476, 476, 480, 480, 480, 515, 530, 530, 530, 546, 619, 626, 670, 723, 838, 973, 1029, 1111, 1247, 1334, 1385, 1415, 1622, 1920, 2025, 2071, 2223, 3766, 3785, 3941, 4468, 4472, 4536, 4613, 5244, 5345, 5535, 5952, 7486]\n"
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(25000,)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 0, 0, 1, 0])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# 查看标签数据前五个的数组长度\n",
    "train_labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入单词索引,是一个字典形式\n",
    "word_index = imdb.get_word_index(path=path_tmp+'imdb_word_index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "type(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inverse_word_index ={}\n",
    "# for key,val in word_index.items():\n",
    "#     inverse_word_index[val] = key\n",
    "\n",
    "# # 按照值进行排序\n",
    "# inverse_word_index = sorted(inverse_word_index.items(), key = lambda k: k[0])\n",
    "\n",
    "# # 重新存为一个新字典\n",
    "# new_word_index = {}\n",
    "# for i in inverse_word_index:\n",
    "#     new_word_index[i[0]] =i[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "of , 4\na , 3\nthe , 1\nand , 2\n"
    }
   ],
   "source": [
    "# 查看字典的前几个索引对应的单词\n",
    "for key,val in word_index.items():\n",
    "    if val < 5:\n",
    "        print(key,',',val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict(\n",
    "    (value, key) for (key, value) in word_index.items()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review_0 = ' '.join(\n",
    "    reverse_word_index.get(i - 3, '?') for i in train_data[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"? ? ? ? ? ? ? the the the the the the the the the the the the the the the and and and and and and and and and a a a of of to to to is it it it it it it i i i this this this that that that that was was was was was was was was was was was as as as for for for with with but film film film film film film you you you you are are are have have be all all all at at an they they they they so so so so from just just just just out if there there what what good up would story story really really were were much been also great because because don't them could after think think watch two being being life little know end these say such should real now director same same part us fact big must done whole whole played played true actor play everyone left father stars came recommend often definitely loved direction throughout children children amazing amazing amazing soon brilliant brilliant brilliant myself sad released robert paul imagine casting list island bought lovely scenery cry location witty connection grown fly norman cried suited everyone's someone's scottish remarks fishing shared praised boy's profile\""
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# 查看第一条评论，通过翻译可知，这是一条正面积极的评论，label为1\n",
    "decoded_review_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review_1 = ' '.join(\n",
    "    reverse_word_index.get(i - 3, '?') for i in train_data[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "\"? big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal ? the hair is big lots of boobs ? men wear those cut ? shirts that show off their ? sickening that men actually wore them and the music is just ? trash that plays over and over again in almost every scene there is trashy music boobs and ? taking away bodies and the gym still doesn't close for ? all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# 查看第二条评论可以知道，这是一条负面的评论，label为0\n",
    "decoded_review_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    " random_test = np.random.random((3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.27592385, 0.72894211],\n       [0.03599346, 0.92322249],\n       [0.00794755, 0.98331935]])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "random_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.03599346, 0.92322249, 0.92322249])"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "random_test[1,[0,1,1]] # 取第二行的第1个和第2个数据,这个位置方便理解下面的vectorize_sequences函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test[1,[0,1]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.27592385, 0.72894211],\n       [1.        , 1.        ],\n       [0.00794755, 0.98331935]])"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "random_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.275923854027733"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "random_test[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　使用one_hot编码的形式对整数列表转换为张量,\n",
    "def vectorize_sequences(sequences,dimension = 10000):\n",
    "    '''\n",
    "    函数名：向量化序列\n",
    "    sequences： 输入列表\n",
    "    dimension：维度\n",
    "    '''\n",
    "    results = np.zeros((len(sequences), dimension)) # 创建一个序列数据样本数等同的行数，10000列数的0矩阵\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.  # 有数据的地方标注为0,其他取不出数据的地方默认为0不变\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 \n [1, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8, 8, 9, 12, 12, 12, 12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 21, 22, 22, 22, 22, 22, 22, 25, 25, 25, 25, 26, 26, 26, 28, 28, 30, 32, 32, 32, 33, 33, 35, 36, 36, 36, 36, 38, 38, 38, 38, 39, 43, 43, 43, 43, 46, 48, 50, 50, 51, 51, 52, 56, 62, 65, 65, 66, 66, 71, 71, 76, 77, 82, 87, 88, 88, 92, 98, 100, 103, 104, 104, 106, 107, 112, 112, 113, 117, 124, 130, 134, 135, 141, 144, 147, 150, 167, 172, 172, 173, 178, 192, 194, 215, 224, 226, 226, 256, 256, 283, 284, 297, 316, 317, 336, 381, 385, 386, 400, 407, 447, 458, 469, 476, 476, 480, 480, 480, 515, 530, 530, 530, 546, 619, 626, 670, 723, 838, 973, 1029, 1111, 1247, 1334, 1385, 1415, 1622, 1920, 2025, 2071, 2223, 3766, 3785, 3941, 4468, 4472, 4536, 4613, 5244, 5345, 5535, 5952, 7486]\n"
    }
   ],
   "source": [
    "for i, sequence in enumerate(train_data[0:1]):\n",
    "    print(i, '\\n', sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 相当于在每一行数据中都是一个one_hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 1., 1., ..., 0., 0., 0.])"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(25000, 10000)"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 0, 0, ..., 0, 1, 0])"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_temp =  vectorize_sequences(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(218, 10000)"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "x_train_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "x_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "x_train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 还需要自己建造一个短的数组来理解这个问题\n",
    "train_sample = [\n",
    "    [23,14,22,11],\n",
    "    [8,1,2,11],\n",
    "    [4,3,4,4],\n",
    "    [3,1,22,11,2],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension的设置必须大于train_sample中最大的数\n",
    "x_train_sample = vectorize_sequences(train_sample, dimension = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "x_train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[4, 3, 4, 4]"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "train_sample[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "x_train_sample[2]"
   ]
  },
  {
   "source": [
    "##### 实验证明，这样的处理方式就会导致如果存在一个词语若在一个句子中存在重复单词的情况，数据处理后只会按照一个单词位置存储，不会体现词语的重复性特征\n",
    "\n",
    "该点存疑，后期再回顾确认？？？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 2.通过文本更加深入理解one_hot编码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['the cat sat on the mat.', 'the dog ate my homework.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['the', 'cat', 'sat', 'on', 'the', 'mat.']"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "samples[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split(): # 默认用空格分词，实际应用中还需要去掉标点和特殊字符\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1 # 为每一个唯一单词指定一个唯一索引，注意索引0未指定单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'the': 1,\n 'cat': 2,\n 'sat': 3,\n 'on': 4,\n 'mat.': 5,\n 'dog': 6,\n 'ate': 7,\n 'my': 8,\n 'homework.': 9}"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[0., 0., 0.],\n        [0., 0., 0.]],\n\n       [[0., 0., 0.],\n        [0., 0., 0.]]])"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "# 生成一个张量的示例\n",
    "np.zeros(shape = (2,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros(\n",
    "    shape = (len(samples),\n",
    "            max_length,\n",
    "            max(token_index.values()) + 1 )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j , word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2, 10, 10)"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    " # i来表示这个长句子所处的张量的位置，j表示这个张量的横轴位置，index表示这个张量的纵轴位置，通过这样映射关系来确定单词在one_hot中的位置\n",
    " results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n\n       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "results[0,[0]]  # the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "results[0,[1]] # cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "results[0,[2]] # sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "results[0,[3]] # on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "results[0,[4]] # the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "results[0,[5]] # mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "results[0,[6]] # 从6-9都是全部填充为0"
   ]
  },
  {
   "source": [
    "##### 总结和需要注意的点\n",
    "- 构建一个固定长度的张量，对每一个单词都映射一个one_hot编码来表示单词，重复的地方仍然重复表示\n",
    "- 如果缺位的地方，是全部用0填充\n",
    "- 这个方法有个缺点，就是当词表中的单词唯一值过大，会导致编码的张量横轴维度很大"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### 3.通过散列编码解决one_hot的单词唯一值太大问题\n",
    "- 散列编码实质上是对一个单词求一个固定的hash长度，然后对这个长度除以张量的维度求余，这样的求余数可能会导致不同单词的散列编码相同"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['the cat sat on the mat.', 'the dog ate my homework.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 1000 # 定义单词保存长度为1000，如果单词数据接近1000个，那么会遭遇散列冲突，就是不同的单词可能由同一个散列值表示\n",
    "max_length = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((len(samples), max_length, dimensionality))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8533866751935312883"
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "# 哈希（hash）也翻译作散列。Hash算法，是将一个不定长的输入，通过散列函数变换成一个定长的输出，即散列值\n",
    " \n",
    "# hash函数可以生成任意字符串或者数值的哈希值，且当更换环境后（比如换一个jupyter），同一个字符串的hash将改变\n",
    "hash('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "19"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "len(str(hash('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "len(str(hash('test1')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "20 % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "10020 % 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word)) % dimensionality #求余数，这样就可以取到一个相近的表示\n",
    "        results[i, j, index] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]])"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2, 10, 1000)"
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "source": [
    "#### 4.通过Embedding词嵌入方法将单词转换为低维浮点数向量\n",
    "- 第一步的做法是统计所有的单词的出现频率，并按照频率做降序排序，然后选择TOP N个常见单词，如N = 10000，其他的剔除\n",
    "- 将每一条文本的单词长度限定为M，M∈N，例如 'i love you so much'只选择'i love you'（因为so much并不在TOP N中）\n",
    "- 实例化M的长度为20，则'i love you'缺失的长度全部由0填充，如果一条文本的长度超出20，则做截断处理"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import preprocessing\n",
    "max_features = 10000 # 定义TOP N\n",
    "maxlen = 20 # 定义M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据的本地导入\n",
    "(x_train, y_train), (x_test, y_testb) = imdb.load_data(path=path_tmp+'imdb.npz',\n",
    "num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整数列表提取为长度只有20的关键词张量\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen = maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[  65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n   15   16 5345   19  178   32]\n"
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(20,)"
     },
     "metadata": {},
     "execution_count": 130
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(25000, 20)"
     },
     "metadata": {},
     "execution_count": 129
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "source": [
    "##### 使用Embedding层和分类器"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten层用来将输入“压平”，即把多维的输入一维化\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen)) #指定Embedding层的最大输入长度，以便后面将嵌入输入展平，Embedding层激活的形状为（samples, maxlen, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten()) # 将三维的嵌入张量展平为形状为（samples, maxlen * 8）的二维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid')) # 在上面添加分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 20, 8)             80000     \n_________________________________________________________________\nflatten (Flatten)            (None, 160)               0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 161       \n=================================================================\nTotal params: 80,161\nTrainable params: 80,161\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss = 'binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.2917 - acc: 0.8816 - val_loss: 0.5270 - val_acc: 0.7488\nEpoch 2/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.2759 - acc: 0.8895 - val_loss: 0.5340 - val_acc: 0.7448\nEpoch 3/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.2604 - acc: 0.8971 - val_loss: 0.5431 - val_acc: 0.7414\nEpoch 4/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.2452 - acc: 0.9050 - val_loss: 0.5535 - val_acc: 0.7400\nEpoch 5/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.2310 - acc: 0.9118 - val_loss: 0.5647 - val_acc: 0.7376\nEpoch 6/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.2174 - acc: 0.9193 - val_loss: 0.5775 - val_acc: 0.7340\nEpoch 7/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.2047 - acc: 0.9251 - val_loss: 0.5880 - val_acc: 0.7342\nEpoch 8/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.1927 - acc: 0.9299 - val_loss: 0.6002 - val_acc: 0.7310\nEpoch 9/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.1814 - acc: 0.9358 - val_loss: 0.6147 - val_acc: 0.7286\nEpoch 10/10\n625/625 [==============================] - 2s 4ms/step - loss: 0.1710 - acc: 0.9400 - val_loss: 0.6295 - val_acc: 0.7274\n"
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}