{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一章第一节  简介"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T02:46:36.807746Z",
     "start_time": "2021-02-14T02:46:29.848814Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import jieba  # 中文分词库，百度员工开发\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "# 查询系统可用的 GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "# 确保有可用的 GPU 如果没有, 则会报错\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# 设置参数,该段务必在运行jupyter的第一段代码执行，否则会无法初始化成功\n",
    "# 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow2.0相比较1.0在API接口上调用更加易用，将Keras收购后完全融入为新的子接口做深度学习调用tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(15, shape=(), dtype=int16)\n"
     ]
    }
   ],
   "source": [
    "# 通过tensorflow计算常量示例\n",
    "a = tf.constant(3,dtype='int16')\n",
    "b = tf.constant(5,dtype='int16')\n",
    "c = a*b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[3, 3, 3],\n",
       "       [3, 3, 3]], dtype=int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "a = tf.constant(3,shape=(2,3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(20, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant(5) * tf.constant(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tensorflow装饰器调用展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def simple_nn_layer(x,y):\n",
    "    return tf.nn.relu(tf.matmul(x,y)) # 矩阵乘法，而不是元素乘法\n",
    "\n",
    "x = tf.random.uniform(shape=(3,3),maxval=100,dtype = tf.int32) # 生成一个3*3的随机实数2D张量\n",
    "y = tf.random.uniform(shape=(3,3),maxval=3,dtype = tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[35,  6, 97],\n",
       "       [28, 21, 81],\n",
       "       [ 8, 16, 67]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[0, 0, 0],\n",
       "       [0, 2, 1],\n",
       "       [1, 2, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[ 97, 206,   6],\n",
       "       [ 81, 204,  21],\n",
       "       [ 67, 166,  16]], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn_layer(x,y) # 通过装饰器调用图算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([35,  6, 97], dtype=int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "206\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "tf.print(sum(x[0] * y[:,0]))\n",
    "tf.print(sum(x[0] * y[:,1]))\n",
    "tf.print(sum(x[0] * y[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常用tf接口"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![常用tf接口](./markdown_pics/tf常用接口.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、TensorFlow低阶API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![常用tf接口](./markdown_pics/tf低阶API.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 6,  8],\n",
       "       [10, 12]], dtype=int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 低阶API举例\n",
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[5,6],[7,8]])\n",
    "\n",
    "# 标量计算 a+b\n",
    "tf.math.add(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[-4, -4],\n",
       "       [-4, -4]], dtype=int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量计算 a-b\n",
    "tf.math.subtract(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 5, 12],\n",
       "       [21, 32]], dtype=int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量计算 a*b 乘法\n",
    "tf.math.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
       "array([[0.2       , 0.33333333],\n",
       "       [0.42857143, 0.5       ]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 标量计算 a/b 除法\n",
    "tf.math.divide(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量计算\n",
    "a = tf.range(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(99,), dtype=int32, numpy=\n",
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4950\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.reduce_sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.reduce_mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.reduce_max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.reduce_min(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.reduce_prod(a)) #此函数计算一个张量的各个维度上元素的乘积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、TensorFlow中阶API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![常用tf接口](./markdown_pics/tf中阶API.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中阶\n",
    "a = tf.random.uniform(shape=(10,5),minval=0,maxval = 10) #生成10乘以5的均匀分布的张量，均匀分布会定义一个最大值和最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 5), dtype=float32, numpy=\n",
       "array([[0.17774701, 2.9730237 , 6.28574   , 3.551135  , 7.1986284 ],\n",
       "       [8.255257  , 7.9624367 , 6.452936  , 7.8144827 , 3.1595635 ],\n",
       "       [6.4324865 , 3.4416294 , 8.746914  , 6.3547287 , 1.7798173 ],\n",
       "       [0.30650258, 3.6757684 , 6.609447  , 5.704936  , 7.5399947 ],\n",
       "       [5.03374   , 9.812571  , 1.6079056 , 4.5578146 , 1.7115128 ],\n",
       "       [8.427334  , 4.987854  , 4.3054047 , 6.791686  , 6.7254043 ],\n",
       "       [4.7570314 , 6.6888785 , 9.703931  , 4.2203655 , 6.257417  ],\n",
       "       [5.5932426 , 1.81211   , 2.5132215 , 1.3635194 , 0.23848176],\n",
       "       [2.7033746 , 0.7865381 , 8.2957945 , 8.912367  , 5.2475014 ],\n",
       "       [3.4562147 , 7.5318966 , 7.8864    , 3.1740856 , 8.271439  ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 5), dtype=float32, numpy=\n",
       "array([[6.18906226e-04, 1.01297349e-02, 2.78158635e-01, 1.80579629e-02,\n",
       "        6.93034768e-01],\n",
       "       [3.90513122e-01, 2.91383892e-01, 6.44017681e-02, 2.51310080e-01,\n",
       "        2.39113579e-03],\n",
       "       [8.26166645e-02, 4.15102066e-03, 8.36008489e-01, 7.64359832e-02,\n",
       "        7.87842087e-04],\n",
       "       [4.58222668e-04, 1.33146588e-02, 2.50270963e-01, 1.01294607e-01,\n",
       "        6.34661555e-01],\n",
       "       [8.28809012e-03, 9.85993862e-01, 2.69546319e-04, 5.14947157e-03,\n",
       "        2.98971252e-04],\n",
       "       [7.01531887e-01, 2.25061029e-02, 1.13740815e-02, 1.36676744e-01,\n",
       "        1.27911255e-01],\n",
       "       [6.50583068e-03, 4.49048504e-02, 9.15617049e-01, 3.80392303e-03,\n",
       "        2.91683618e-02],\n",
       "       [9.19086277e-01, 2.09522769e-02, 4.22396287e-02, 1.33786080e-02,\n",
       "        4.34323261e-03],\n",
       "       [1.28293422e-03, 1.88683000e-04, 3.44319284e-01, 6.37874663e-01,\n",
       "        1.63344499e-02],\n",
       "       [3.73181445e-03, 2.19768837e-01, 3.13274533e-01, 2.81444564e-03,\n",
       "        4.60410416e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    " a = tf.random.uniform(shape=(10,5),minval = -0.5, maxval = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([10, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "b = tf.keras.layers.Dense(10)(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "做线性变换前的维度\n",
      " TensorShape([10, 5])\n",
      "做线性变换后的维度\n",
      " TensorShape([10, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584],\n",
       "       [0.1970199 , 0.06478176, 0.08340064, 0.05789879, 0.05120848,\n",
       "        0.07919518, 0.12206888, 0.10725715, 0.13625334, 0.10091584]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.uniform(shape=(10,5),minval = -0.5,maxval = 0.5)\n",
    "tf.print('做线性变换前的维度\\n',a.shape)\n",
    "b = tf.keras.layers.Dense(10)(a)\n",
    "tf.print('做线性变换后的维度\\n',b.shape)\n",
    "b = tf.nn.softmax(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635],\n",
       "       [0.1415823 , 0.10353278, 0.09819841, 0.08567477, 0.06916812,\n",
       "        0.13720214, 0.11492527, 0.07812237, 0.08605745, 0.08553635]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.uniform(shape=(10,100,50),minval = -0.5,maxval = -0.5)\n",
    "a = tf.keras.layers.LSTM(100)(a)\n",
    "b = tf.keras.layers.Dense(10)(a)\n",
    "b = tf.nn.softmax(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、TensorFlow高阶API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![常用tf接口](./markdown_pics/tf高阶API.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高阶API应用\n",
    "X = tf.random.uniform(shape=(10,100,50),minval = -0.5,maxval = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.random.categorical(tf.random.uniform(shape=(10,3),minval = -0.5,maxval = 0.5),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.LSTM(100,input_shape = (100,50)))\n",
    "model.add(tf.keras.layers.Dense(10))\n",
    "model.add(tf.keras.layers.Dense(2,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 100)               60400     \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                1010      \n_________________________________________________________________\ndense_4 (Dense)              (None, 2)                 22        \n=================================================================\nTotal params: 61,432\nTrainable params: 61,432\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 1.5779 - accuracy: 0.3000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f45906414c0>"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![常用tf接口](./markdown_pics/tfAPI总结.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本章完结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.待掌握softmax函数的原理\n",
    "#### 2.tf.keras.layers.Dense 的具体原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1., 2., 3., 4., 5.]])\n",
    "\n",
    "# 这里表示的w\n",
    "kernel =  np.array([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]\n",
    " , [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]\n",
    " , [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]\n",
    " , [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]\n",
    " , [ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]])\n",
    "\n",
    "b =  np.array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5) (5, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape, kernel.shape, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float64, numpy=array([[ 15.,  30.,  45.,  60.,  75.,  90., 105., 120., 135., 150.]])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(inputs,kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float64, numpy=array([[ 15.,  31.,  47.,  63.,  79.,  95., 111., 127., 143., 159.]])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(inputs,kernel) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T02:49:28.420889Z",
     "start_time": "2021-02-14T02:49:28.411476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "tf.print(tf.reduce_max(tf.constant(1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T02:51:17.500950Z",
     "start_time": "2021-02-14T02:51:17.493135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(tf.constant(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T02:46:57.370652Z",
     "start_time": "2021-02-14T02:46:57.361507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reduce_max in module tensorflow.python.ops.math_ops:\n",
      "\n",
      "reduce_max(input_tensor, axis=None, keepdims=False, name=None)\n",
      "    Computes the maximum of elements across dimensions of a tensor.\n",
      "    \n",
      "    Reduces `input_tensor` along the dimensions given in `axis`.\n",
      "    Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each\n",
      "    entry in `axis`. If `keepdims` is true, the reduced dimensions\n",
      "    are retained with length 1.\n",
      "    \n",
      "    If `axis` is None, all dimensions are reduced, and a\n",
      "    tensor with a single element is returned.\n",
      "    \n",
      "    Usage example:\n",
      "    \n",
      "    >>> x = tf.constant([5, 1, 2, 4])\n",
      "    >>> print(tf.reduce_max(x))\n",
      "    tf.Tensor(5, shape=(), dtype=int32)\n",
      "    >>> x = tf.constant([-5, -1, -2, -4])\n",
      "    >>> print(tf.reduce_max(x))\n",
      "    tf.Tensor(-1, shape=(), dtype=int32)\n",
      "    >>> x = tf.constant([4, float('nan')])\n",
      "    >>> print(tf.reduce_max(x))\n",
      "    tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "    >>> x = tf.constant([float('nan'), float('nan')])\n",
      "    >>> print(tf.reduce_max(x))\n",
      "    tf.Tensor(-inf, shape=(), dtype=float32)\n",
      "    >>> x = tf.constant([float('-inf'), float('inf')])\n",
      "    >>> print(tf.reduce_max(x))\n",
      "    tf.Tensor(inf, shape=(), dtype=float32)\n",
      "    \n",
      "    See the numpy docs for `np.amax` and `np.nanmax` behavior.\n",
      "    \n",
      "    Args:\n",
      "      input_tensor: The tensor to reduce. Should have real numeric type.\n",
      "      axis: The dimensions to reduce. If `None` (the default), reduces all\n",
      "        dimensions. Must be in the range `[-rank(input_tensor),\n",
      "        rank(input_tensor))`.\n",
      "      keepdims: If true, retains reduced dimensions with length 1.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      The reduced tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reduce_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 对Dense层做最基础的测试试图理解内部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      Losses which are associated with this `Layer`.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of `tf.keras.metrics.Metric` instances tracked by the layer.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.keras.layers.Dense(units=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import jieba  # 中文分词库，百度员工开发\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "# # 查询系统可用的 GPU\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# # 确保有可用的 GPU 如果没有, 则会报错\n",
    "# assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# # 设置参数,该段务必在运行jupyter的第一段代码执行，否则会无法初始化成功\n",
    "# # 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[1 2]\n",
      " [2 2]]\n",
      "y [[0 1]\n",
      " [1 0]]\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 2)                 6         \n",
      "=================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7530 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f89280af910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2],[2,2]])\n",
    "print('X',X)\n",
    "y = np.array([1,0])\n",
    "y = tf.one_hot(y,2)\n",
    "tf.print('y',y)\n",
    "model = tf.keras.models.Sequential()\n",
    "# model.add(tf.keras.layers.Dense(units= 2,input_shape = (2,)))\n",
    "# units= 2 该层的神经元个数\n",
    "model.add(tf.keras.layers.Dense(units= 2, input_shape = (2,) ,activation = 'softmax'))\n",
    "\n",
    "# 使用随机梯度下降作为优化器 、交叉熵损失函数、 准确率评估\n",
    "model.compile(optimizer='sgd',loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此处着重还原Dense的处理过程\n",
    "\n",
    "# 随机初始化 w、b\n",
    "w = np.array([[0.1,0.3],[0.4,0.7]])\n",
    "b = np.array([1,2])\n",
    "\n",
    "c = np.dot(w, X) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
       "array([[0.24973989, 0.75026011],\n",
       "       [0.19781611, 0.80218389]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将结果做softmax转换\n",
    "tf.nn.softmax(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成预测结果\n",
    "pred = [[0,1],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 真实结果\n",
    "y = [[0,1],[1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确率 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 手写一个SoftMax计算公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def softMax(Trans_array:list):\n",
    "    if len(Trans_array) == 0:\n",
    "        return Trans_array\n",
    "    sum_i = 0\n",
    "    for i in range(len(Trans_array)):\n",
    "        Trans_array[i]  = np.exp(Trans_array[i])\n",
    "        sum_i += Trans_array[i]\n",
    "    for log_i in range(len(Trans_array)):\n",
    "        Trans_array[log_i] = round(Trans_array[log_i]/sum_i,4)\n",
    "    return Trans_array, round(sum(Trans_array),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.042, 0.1142, 0.8438], 1.0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softMax([2,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果使用对数\n",
    "import numpy as np \n",
    "def softMax(Trans_array:list):\n",
    "    if len(Trans_array) == 0:\n",
    "        return Trans_array\n",
    "    sum_i = 0\n",
    "    for i in range(len(Trans_array)):\n",
    "        Trans_array[i]  = np.log(Trans_array[i])\n",
    "        sum_i += Trans_array[i]\n",
    "    for log_i in range(len(Trans_array)):\n",
    "        Trans_array[log_i] = round(Trans_array[log_i]/sum_i,4)\n",
    "    return Trans_array, round(sum(Trans_array),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([0.2038, 0.323, 0.4732], 1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "softMax([2,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([123, 456, 789])\n",
    "scores -= np.max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-666, -333,    0])"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-1.5390596 ,  0.5894602 , -0.55105644, -0.3392847 ,  0.5044844 ,\n",
       "         1.0958196 , -0.30700386,  0.12144363, -2.0147839 ,  0.55208945],\n",
       "       [ 0.31052962, -1.2160761 ,  1.764273  ,  0.13621946,  2.264149  ,\n",
       "         0.8540781 ,  0.24092601, -0.06370308, -2.3819096 , -0.2245071 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "z = tf.random.normal([2,10])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tf.random.normal([2, 10]) # 构造2个样本的10类别输出的输出值\n",
    "y = tf.constant([1, 3]) # 两个样本的真实样本标签是1和3\n",
    "y_true = tf.one_hot(y, depth = 10) # 构造onehot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 1.0583026 ,  0.1122171 ,  0.63292325,  1.0119021 ,  0.8550314 ,\n",
       "        -1.0603844 ,  0.5451556 ,  1.5006305 , -0.13813645, -0.66283834],\n",
       "       [ 0.65085083,  0.1896415 , -0.7920322 , -0.7588749 , -0.44227013,\n",
       "        -0.32016006, -0.12084924,  1.1455139 , -0.1418101 ,  0.5171825 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = tf.keras.losses.categorical_crossentropy(y_true, z, from_logits = True)\n",
    "loss1 = tf.reduce_mean(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.035721>"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[0.15223597, 0.0591068 , 0.09948952, 0.1453335 , 0.1242332 ,\n",
       "        0.01829714, 0.09112978, 0.23692876, 0.04601616, 0.02722922],\n",
       "       [0.15988393, 0.10081013, 0.03777189, 0.0390453 , 0.05358811,\n",
       "        0.06054805, 0.0739026 , 0.26220095, 0.07236966, 0.1398793 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(3.0357208, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 输出层经过Softmax激活函数,因此将from_logits设置为True\n",
    "loss2 = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits = False)\n",
    "loss2 = tf.reduce_mean(loss2)\n",
    "print(loss2) # tf.Tensor(2.668019, shape=(), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38364bitcb11fcd8cdb441e4bf52994f94e346da",
   "display_name": "Python 3.8.3 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}