{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "# 查询系统可用的 GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "# 确保有可用的 GPU 如果没有, 则会报错\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# 设置参数,该段务必在运行jupyter的第一段代码执行，否则会无法初始化成功\n",
    "# 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.数据处理\n",
    "- 数据shuffle\n",
    "- TFRecord方式\n",
    "- TextLineDataset方式\n",
    "- from_generator方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![通过对大数据进行拆分成小批量数据的过程](./markdown_pics/通过对大数据进行拆分成小批量数据的过程.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Simplified('./data')\n",
    "NCSVS = 100\n",
    "\n",
    " # 每个文件都分成100份，然后将每个文件的1/100的数据写入内存,这样就能确保内存不会溢出\n",
    "categories = s.list_all_categories()\n",
    "print(len(categories))\n",
    "\n",
    "for y, cat in tqdm(enumerate(categories)):\n",
    "    df = s.read_training_csv(cat)\n",
    "    df['y']  = y\n",
    "    df['cv'] = (df.key_id // 10 ** 7) % NCSVS  \n",
    "    \n",
    "    for k in range(NCSVS):\n",
    "        filename = f'./shuffle_data/train_k{k}.csv'\n",
    "        chunk = df[df.cv == k]\n",
    "        chunk = chunk.drop(['key_id'], axis = 1)\n",
    "\n",
    "        if y == 0 :\n",
    "            chunk.to_csv(filename, index = False)\n",
    "        else:\n",
    "            chunk.to_csv(filename, mode = 'a', header = False, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结一下小批量读取大数据的形式：\n",
    "##### 1. 对本赛题的340个文件，每一个，都按照100等份进行拆分，拆分后的文件分别有3400个\n",
    "##### 2.然后对每个类别的文件的前1/100的数据做合并，然后按照标签做随机化，房子模型读取的数据趋同\n",
    "##### 3.对随机化后的340个小文件做合并并做压缩，然后作为模型的第一部分的小数据输入，这样就解决内存溢出问题\n",
    "##### 4.接着是模型重复的输入第2/100的数据，以此类推。模型一共需要迭代100次才能输入全部的数据。\n",
    "##### 5.可以通过上图查看到最终的文件样式和对应的编号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFRecord方式\n",
    "- 按照上图的文件类型分割后，仅拿train_k0.csv.gz文件而言，生成的tfrecord的文件大约为7.7GB。\n",
    "- 对于所有的train_k{}csv.gz文件，这样算下来，大概为770GB，因此需要消耗巨大的硬盘空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(tfrecord_file) as writer:\n",
    "    for filename in fileList[:1]:\n",
    "        df = pd.read_csv(filename)\n",
    "        df['drawing'] = df['drawing'].apply(json.loads)\n",
    "\n",
    "        for row in range(df.shape[0]):\n",
    "            drawing = df.loc[row, 'drawing']\n",
    "            img = draw_cv2(drawing, BASE_SIZE = 128, size = 128, lw = 6) # 构建一个图像\n",
    "            img = img.tostring()\n",
    "            label = df.loc[row, 'y']\n",
    "\n",
    "        # 建立tf.train.Feature 字典\n",
    "        feature = {\n",
    "            'image': tf.train.Feature(bytes_list = tf.train.BytesList(value = [img])),  # 图片是一个 Bytes 对象\n",
    "            'label': tf.train.Feature(int64_list = tf.train.Int64List(value = [label])) # 标签是一个 Int 对象\n",
    "        }\n",
    "        example = tf.train.Example(feature = tf.train.Features(feature = feature)) # 通过字典建立Example\n",
    "        writer.writer(example.SerializePartialToString()) # 将Example序列化并写入TFRecord文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TextLineDataset方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cv2(raw_strokes, size = 64, lw = 6):\n",
    "    raw_strokes = eval(raw_strokes.numpy())\n",
    "    img = np.zeros((256,256), np.uint8)\n",
    "    \n",
    "    for stroke in raw_strokes:\n",
    "        for i in range(len(stroke[0]) - 1):\n",
    "            _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i+1], stroke[1][i+1]), 255, lw)\n",
    "    return cv2.resize(img, (size, size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_draw_cv2(image, label):\n",
    "    [image] = tf.py_function(draw_cv2, [image], [tf.float32])\n",
    "    image = tf.reshape(image, (64,64,1))\n",
    "    label = tf.one_hot(label, depth = NCATS)\n",
    "    image.set_shape((64, 64, 1))\n",
    "    label.set_shape((340,))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.TextLineDataset(fileList[2], compression_type= 'GZIP').skip(1).map(parse_csv, num_parallel_calls= tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.map(tf_draw_cv2, num_parallel_calls= tf.data.experimental.AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE).shuffle(3000).batch(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![py_function的作用](./markdown_pics/py_function的作用.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![py_function的解释](./markdown_pics/py_function的解释.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from_generator方法\n",
    "- tf.data.Dataset类提供from_generator方法，针对大数据量比较友好，难点是gen必须是一个可调用的对象，返回支持iter()对象的协议"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, array([1])), (2, array([1, 1])), (3, array([1, 1, 1]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def gen():\n",
    "    for i in itertools.count(start = 1):\n",
    "        print(i)\n",
    "        yield(i, [1] * i)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    (tf.int64, tf.int64),\n",
    "    (tf.TensorShape([]), tf.TensorShape([None]))\n",
    ")\n",
    "\n",
    "list(dataset.take(3).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo():\n",
    "    print(\"starting...\")\n",
    "    while True:\n",
    "        res = yield 4\n",
    "        print(\"res:\",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = foo() # 执行函数调用后，函数没有输出任何内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(next(g)) # 结合next的时候，才开始生效, 且yield作为return的作用返回了4，但是第二个print并没有生效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "res: None\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(\"*\"*20) \n",
    "print(next(g)) #从这里可以看出，再一次迭代后，开始执行的是yield后面的语句，相当于接着上面一个执行未执行完毕的部分，但是res并没拿到 4这个值\n",
    "# 然后执行完毕，又开始执行yield的return部分，然后执行又结束了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![from_generator方法参数](./markdown_pics/from_generator方法参数.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "class Dataloader(object):\n",
    "    def __init__(self, resize_height =64, resize_width = 64, batch_size = 512, fileList = None, size = 256, lw = 6):\n",
    "        self.resize_height = resize_height # 图片的高度\n",
    "        self.resize_width = resize_width # 图片的宽度\n",
    "        self.batch_size = batch_size # batch大小\n",
    "        self.fileList = fileList # 文件数据\n",
    "        self.size = size # 画图时图片的大小\n",
    "        self.lw = lw\n",
    "\n",
    "    def __call__(self):\n",
    "        def _generator(size, lw):\n",
    "            while True:\n",
    "                for filename in np.random.permutation(self.fileList):\n",
    "                    df = pd.read_csv(filename)\n",
    "                df['drawing'] = df['drawing'].apply(json.loads) # 转换一下数据格式\n",
    "                x = np.zeros((len(df), size, size))\n",
    "\n",
    "                for i, raw_strokes in enumerate(df.drawing.values):\n",
    "                    x[i] = draw_cv2(raw_strokes, size =size, lw = lw)\n",
    "                \n",
    "                x = x/ 255.0\n",
    "                x = x.reshape((len(df), size, size,1)).astype(np.float32)  #这里转换为黑白图像\n",
    "                y = tf.keras.utils.to_categorical(df.y, num_classes= n_labels)\n",
    "\n",
    "                for x_i, y_i in zip(x,y):\n",
    "                    yield(x_i, y_i)\n",
    "\n",
    "\n",
    "\n",
    "                dataset = tf.data.Dataset.from_generator(generator = _generator,\n",
    "                    output_types=(tf.dtypes.float32, tf.dtypes.float32),\n",
    "                    output_shapes=((self.resize_height, self.resize_width, 1), (340, )),\n",
    "                    args=(self.size, self.lw)\n",
    "                )\n",
    "\n",
    "                dataset = dataset.prefetch(buffer_size = 10240)\n",
    "                dataset = dataset.shuffle(buffer_size = 10240).batch(self.batch_size)\n",
    "\n",
    "                return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.建模方法\n",
    "- 预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![预训练的建模方法](./markdown_pics/预训练的建模方法.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 针对不同的场景，使用不同的预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![预训练模型的列表](./markdown_pics/预训练模型的列表.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![预训练模型调用的方法举例](./markdown_pics/预训练模型调用的方法举例.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![预训练模型的详细参数1](./markdown_pics/预训练模型的详细参数1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![预训练模型的详细参数2](./markdown_pics/预训练模型的详细参数2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Baseline构建\n",
    "- 数据处理\n",
    "- 数据读取\n",
    "- 建模方法\n",
    "- 参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![卷积神经网络baseline构建的要点](./markdown_pics/卷积神经网络baseline构建的要点.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bitcb11fcd8cdb441e4bf52994f94e346da"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
